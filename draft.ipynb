{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb48e8f-1a56-4eaf-afdb-066fbb4151bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pyhealth inflect autocorrect torchtext gensim==3.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d619733d-803c-4882-884b-6fe1d32fe7da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyhealth.datasets import MIMIC4Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1f4bab-40e2-4ba6-9a8a-5fa3cddd717b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1291cedd-56d1-4549-94e1-f5da8f082ae9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyhealth.datasets import MIMIC4Dataset\n",
    "dataset = MIMIC4Dataset(\n",
    "        root=\"data/mimic4_subset\",\n",
    "        tables=[\"diagnoses_icd\", \"procedures_icd\"],\n",
    "    )\n",
    "dataset.stat()\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e72fb8-896a-4bab-a9af-96fadd5b6aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyhealth.medcode import InnerMap\n",
    "\n",
    "icd9cm = InnerMap.load(\"ICD9CM\")\n",
    "icd10cm = InnerMap.load(\"ICD10CM\")\n",
    "#smoker = icd9cm.lookup(\"V15.82\")\n",
    "\n",
    "counter = 0\n",
    "patient_dict = dataset.patients\n",
    "labels = []\n",
    "for subject_id, patient in patient_dict.items():\n",
    "    #if counter > 100:\n",
    "    #    break\n",
    "    #counter += 1\n",
    "    tobacco = 0\n",
    "    visit_dict = patient.visits\n",
    "    for visit_id, visit in visit_dict.items():\n",
    "        #print(visit.encounter_time, visit.available_tables)\n",
    "        events = visit.get_event_list('diagnoses_icd')\n",
    "        for event in events:\n",
    "            if event.vocabulary == 'ICD9CM' and event.code in ['V1582', '3051']:\n",
    "                tobacco = 1\n",
    "                #explain = icd9cm.lookup(event.code)\n",
    "                #print(event.patient_id, event.visit_id, visit.encounter_time, event.vocabulary, event.code, explain)\n",
    "            elif event.vocabulary == 'ICD10CM' and event.code.startswith('F17'):\n",
    "                tobacco = 1\n",
    "                #explain = icd10cm.lookup(event.code)\n",
    "                #print(event.patient_id, event.visit_id, visit.encounter_time, event.vocabulary, event.code, explain)\n",
    "    labels.append({'subject_id':subject_id,'label':tobacco})\n",
    "                    \n",
    "            \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821bce96-924f-46f1-82c3-d73933ea8963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "import inflect\n",
    "from autocorrect import spell\n",
    "from collections import OrderedDict\n",
    "\n",
    "# function that cleans text\n",
    "# still need to account for contractions, abbreviations, and numbers/fractions\n",
    "default_stemmer = PorterStemmer()\n",
    "default_stopwords = stopwords.words('english') # or any other list of your choice\n",
    "def clean_text(text, replace_numbers = False, remove_rare = False, remove_punctuation = False, stem_text = False, remove_stopwords = False, remove_num = False , spell_check = False, remove_repeat = False):\n",
    "        def misc_cleaning(text):\n",
    "                text = re.sub(\"-([a-zA-Z]+)\", r\"\\1\", text) # replaces hyphen with spaces in case of strings\n",
    "                text = re.sub(' y ', '', text) # gets rid of random y accent stuff scattered through the text\n",
    "                text = re.sub('yyy', 'y', text)\n",
    "                text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "                text = re.sub(r\"what's\", \"what is \", text)\n",
    "                text = re.sub(r\"\\'s\", \" \", text)\n",
    "                text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "                text = re.sub(r\"can't\", \"cannot \", text)\n",
    "                text = re.sub(r\"n't\", \" not \", text)\n",
    "                text = re.sub(r\"i'm\", \"i am \", text)\n",
    "                text = re.sub(r\"\\'re\", \" are \", text)\n",
    "                text = re.sub(r\"\\'d\", \" would \", text)\n",
    "                text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "                text = re.sub(r\",\", \" \", text)\n",
    "                text = re.sub(r\"\\.\", \" \", text)\n",
    "                text = re.sub(r\"!\", \" ! \", text)\n",
    "                text = re.sub(r\"\\/\", \" \", text)\n",
    "                text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "                text = re.sub(r\"\\+\", \" + \", text)\n",
    "                text = re.sub(r\"\\-\", \" - \", text)\n",
    "                text = re.sub(r\"\\=\", \" = \", text)\n",
    "                text = re.sub(r\"'\", \" \", text)\n",
    "                text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "                text = re.sub(r\":\", \" : \", text)\n",
    "                text = re.sub(r\" e g \", \" eg \", text)\n",
    "                text = re.sub(r\" b g \", \" bg \", text)\n",
    "                text = re.sub(r\" u s \", \" american \", text)\n",
    "                text = re.sub(r\"\\0s\", \"0\", text)\n",
    "                text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "                text = re.sub(r\"e - mail\", \"email\", text)\n",
    "                text = re.sub(r\"j k\", \"jk\", text)\n",
    "                text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "                return text\n",
    "\n",
    "        # function to tokenize text which is used in a lot of the later processing\n",
    "        def tokenize_text(text):\n",
    "                return [w for s in sent_tokenize(text) for w in word_tokenize(s)]\n",
    "\n",
    "        text = text.strip(' ') # strip whitespaces\n",
    "        text = misc_cleaning(text) # look at function, random cleaning stuff\n",
    "        text = text.lower() # lowercase\n",
    "\n",
    "        if remove_repeat:\n",
    "                sentences = sent_tokenize(text)\n",
    "                sentences = list(dict.fromkeys(sentences))\n",
    "                text = \" \".join(sentences)\n",
    "        \n",
    "        # removes punctuation\n",
    "        if remove_punctuation:\n",
    "                text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        # optional: replaces numbers (\"3\") with their word counterparts (\"three\")\n",
    "        if replace_numbers:\n",
    "                words = word_tokenize(text)\n",
    "                p = inflect.engine()\n",
    "                new_words = []\n",
    "                for word in words:\n",
    "                        if word.isdigit():\n",
    "                                new_word = p.number_to_words(word)\n",
    "                                new_words.append(new_word)\n",
    "                        else:\n",
    "                                new_words.append(word)\n",
    "                text = \" \".join(new_words)\n",
    "\n",
    "        # optional: removes the rarest words in each text --> right now it's 10\n",
    "        if remove_rare:\n",
    "                tokens = word_tokenize(text)\n",
    "                freq_dist = nltk.FreqDist(tokens)\n",
    "                rarewords = list(freq_dist.keys())[-10:]\n",
    "                new_words = [word for word in tokens if word not in rarewords]\n",
    "                text = \" \".join(new_words)\n",
    "\n",
    "        # optional: stems text using Porter Stemmer\n",
    "        if stem_text:\n",
    "                stemmer = default_stemmer\n",
    "                tokens = tokenize_text(text)\n",
    "                text = \" \".join([stemmer.stem(t) for t in tokens])\n",
    "\n",
    "        # removes stop words such as \"a\", \"the\", etc.\n",
    "        if remove_stopwords:\n",
    "                stop_words = default_stopwords\n",
    "                tokens = [w for w in tokenize_text(text) if w not in stop_words]\n",
    "                text = \" \".join(tokens)\n",
    "        \n",
    "        # optional: removes numbers completely from the ext\n",
    "        if remove_num:\n",
    "                text=text.split()\n",
    "                text=[x for x in text if not x.isnumeric()]\n",
    "                text= \" \".join(text)\n",
    "\n",
    "        return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947d2ec1-217b-4cbd-a21f-f74adab16939",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_notes_discharge = pd.read_csv(\"data/mimic4_notes/discharge.csv\")\n",
    "#df_notes_discharge_detail = pd.read_csv(\"data/mimic4_notes/discharge_detail.csv\")\n",
    "#df_notes_radiology = pd.read_csv(\"data/mimic4_notes/radiology.csv\")\n",
    "\n",
    "print(df_notes_discharge.columns)\n",
    "print('total len', len(df_notes_discharge))\n",
    "#df_notes_discharge_detail.columns\n",
    "\n",
    "#print(df_notes_discharge['subject_id'])\n",
    "#print(df_notes_radiology['text'][0])\n",
    "\n",
    "#group by patient and concatenate all notes for one patient\n",
    "df_notes_discharge = df_notes_discharge.groupby(['subject_id'], as_index = False).agg({'text': ' '.join})\n",
    "print(df_notes_discharge.columns)\n",
    "print('len of patients', len(df_notes_discharge))\n",
    "\n",
    "#limit to 781 patients\n",
    "df_notes_discharge = df_notes_discharge.head(781)\n",
    "print('final len', len(df_notes_discharge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fe066b-7c7d-4e66-a3e0-0f5407bd6505",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.multiprocessing import get\n",
    "\n",
    "#ddf = dd.from_pandas(df_notes_discharge, npartitions=7)\n",
    "#meta_df = pd.DataFrame(columns=[\"subject_id\", \"text\", \"new_text\"], dtype=object)\n",
    "\n",
    "#ddf['text'] = ddf['text'].apply(lambda text: clean_text(text, remove_punctuation = True, remove_stopwords = True, remove_repeat = True))\n",
    "\n",
    "#res = ddf.map_partitions(lambda df: df.assign(new_text = clean_text(df['text'], remove_punctuation = True, remove_stopwords = True, remove_repeat = True)), meta=meta_df)\n",
    "#res.to_csv(\"data/mimic4_notes/discharge_clean.csv\", index=False)\n",
    "\n",
    "#pandas_df = ddf.compute()\n",
    "#pandas_df.to_csv(\"data/mimic4_notes/discharge_clean.csv\", index=False)\n",
    "\n",
    "\n",
    "# without dask\n",
    "df_notes_discharge['text'] = df_notes_discharge['text'].apply(lambda text: clean_text(text, remove_punctuation = True, remove_stopwords = True, remove_repeat = True))\n",
    "\n",
    "#save notes for embeddings\n",
    "notes = list(df_notes_discharge['text'])     \n",
    "\n",
    "# save cleaned notes into a pickle file\n",
    "f = open('data/cleaned_notes.pckl', 'wb')\n",
    "pickle.dump(notes, f)\n",
    "f.close()\n",
    "print(\"Saved cleansed notes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe39ba7-1ac5-4018-b120-0a89fbb61432",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#np.random.seed(1234)\n",
    "#from pyhealth.datasets.splitter import split_by_patient\n",
    "#from pyhealth.datasets import split_by_patient, get_dataloader\n",
    "# data split\n",
    "#train_dataset, val_dataset, test_dataset = split_by_patient(dataset, [0.8, 0.1, 0.1])\n",
    "# create dataloaders (they are <torch.data.DataLoader> object)\n",
    "#train_loader = get_dataloader(train_dataset, batch_size=64, shuffle=True)\n",
    "#val_loader = get_dataloader(val_dataset, batch_size=64, shuffle=False)\n",
    "#test_loader = get_dataloader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f305181b-168a-4dfb-b706-9981ae1e5137",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39e2a04-0959-4cce-b616-dce23b1a7543",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from keras.preprocessing.text import Tokenizer\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import pickle\n",
    "import statistics\n",
    "\n",
    "f = open('data/cleaned_notes.pckl', 'rb')\n",
    "notes = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "def textTokenizeEff(notes):\n",
    "    t = get_tokenizer(\"basic_english\")\n",
    "    max_len = 0\n",
    "    tokenized_texts = []\n",
    "    word2idx = {}\n",
    "\n",
    "    word2idx['<pad>'] = 0\n",
    "    word2idx['<unk>'] = 1\n",
    "    idx = 2\n",
    "    for text in notes:\n",
    "        tokenized_text = t(text)\n",
    "        tokenized_texts.append(tokenized_text)\n",
    "        # Add new token to `word2idx`\n",
    "        for token in tokenized_text:\n",
    "            if token not in word2idx:\n",
    "                word2idx[token] = idx\n",
    "                idx += 1\n",
    "\n",
    "        max_len = max(max_len, len(tokenized_text))\n",
    "    return tokenized_texts, word2idx, max_len\n",
    "    \n",
    "\n",
    "def word_Embed_GNV(word_index):   \n",
    "    pretrain = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "    #convert pretrained word embedding to a dictionary\n",
    "    embedding_index=dict()\n",
    "    for i in range(len(pretrain.wv)):\n",
    "        word=pretrain.wv.index_to_key[i]\n",
    "        if word is not None:\n",
    "            embedding_index[word]=pretrain.wv[word]  \n",
    "    #extract word embedding for train and test data\n",
    "    vocab_size=len(word_index)+1\n",
    "    embedding_matrix = np.zeros((vocab_size, 300))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "def make_w2v_model(notes, window, workers, epochs):\n",
    "    model = gensim.models.Word2Vec(notes, vector_size=300, window=window, min_count=2, workers=workers)\n",
    "    print('Start training process...') \n",
    "    model.train(notes,total_examples=len(notes),epochs=epochs)\n",
    "    model.save(\"w2v.model\")\n",
    "    print(\"Model Saved\")\n",
    "\n",
    "def word_Embed_w2v(word_index, model):   \n",
    "    pretrain = model\n",
    "    #convert pretrained word embedding to a dictionary\n",
    "    embedding_index=dict()\n",
    "    for i in range(len(pretrain.wv)):\n",
    "        word=pretrain.wv.index_to_key[i]\n",
    "        if word is not None:\n",
    "            embedding_index[word]=pretrain.wv[word]  \n",
    "    #extract word embedding for train and test data\n",
    "    vocab_size=len(word_index)+1\n",
    "    embedding_matrix = np.zeros((vocab_size, 300))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "# Execution of variable creation for both whole and efficient\n",
    "#notes_tok, word_index, max_len = textTokenize(notes)\n",
    "notes_tok_eff, word_index_eff, max_len_eff = textTokenizeEff(notes)\n",
    "#embedding_matrix_GNV = word_Embed_GNV(word_index)\n",
    "#embedding_matrix_GNV_eff = word_Embed_GNV(word_index_eff)\n",
    "make_w2v_model(notes, 5, 10, 20)\n",
    "w2v_model = Word2Vec.load(\"w2v.model\")\n",
    "#embedding_matrix_w2v = word_Embed_w2v(word_index, w2v_model)\n",
    "embedding_matrix_w2v_eff = word_Embed_w2v(word_index_eff, w2v_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051fe08f-2838-41d3-b3f6-c96386745d4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = open('data/embeddings/tokenized_notes_eff.pckl', 'wb')\n",
    "pickle.dump(notes_tok_eff, f)\n",
    "f.close()\n",
    "print(\"Saved Tokenized Notes\")\n",
    "\n",
    "#f = open('data/embeddings/embedding_matrix_GNV_eff.pckl', 'wb')\n",
    "#pickle.dump(embedding_matrix_GNV_eff, f)\n",
    "#f.close()\n",
    "#print(\"Saved Google Vector Word Embedding Matrix\")\n",
    "\n",
    "f = open('data/embeddings/embedding_matrix_w2v_eff.pckl', 'wb')\n",
    "pickle.dump(embedding_matrix_w2v_eff, f)\n",
    "f.close()\n",
    "print(\"Saved Word 2 Vector Embedding Matrix\")\n",
    "\n",
    "f = open('data/embeddings/word_index_eff.pckl', 'wb')\n",
    "pickle.dump(word_index_eff, f)\n",
    "f.close()\n",
    "print(\"Saved Word Indices\")\n",
    "\n",
    "f = open('data/embeddings/max_len_eff.pckl', 'wb')\n",
    "pickle.dump(max_len_eff, f)\n",
    "f.close()\n",
    "print(\"Saved Maximum Length of One Patient's Notes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a216987b-b72d-47e2-b1d4-bc24509db3df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(notes_tok_eff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41360dab-19f5-406d-9ecd-099fa40e5ec5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import (TensorDataset, DataLoader, RandomSampler,\n",
    "                              SequentialSampler)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def data_loader(train_inputs, val_inputs, train_labels, val_labels,\n",
    "                batch_size=50):\n",
    "    \"\"\"Convert train and validation sets to torch.Tensors and load them to\n",
    "    DataLoader.a\n",
    "  a  \"\"\"\n",
    "\n",
    "    # Convert data type to torch.Tensor\n",
    "    train_inputs, val_inputs, train_labels, val_labels =\\\n",
    "    tuple(torch.tensor(data) for data in\n",
    "          [train_inputs, val_inputs, train_labels, val_labels])\n",
    "\n",
    "    # Specify batch_size\n",
    "    batch_size = 50\n",
    "\n",
    "    # Create DataLoader for training data\n",
    "    train_data = TensorDataset(train_inputs, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    # Create DataLoader for validation data\n",
    "    val_data = TensorDataset(val_inputs, val_labels)\n",
    "    val_sampler = SequentialSampler(val_data)\n",
    "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "#df_notes_discharge\n",
    "label_df = pd.DataFrame(labels)\n",
    "\n",
    "label_df[\"subject_id\"] = pd.to_numeric(label_df[\"subject_id\"])\n",
    "y = label_df.merge(df_notes_discharge, on='subject_id', how='inner')['label']\n",
    "y = torch.tensor(y.to_numpy())\n",
    "#train_size = int(len(total) * 2/3)\n",
    "#test_size = len(total) - train_size\n",
    "#train_df = total.head(train_size)\n",
    "#test_df = total.tail(test_size)\n",
    "\n",
    "#train = train_df.to_numpy()\n",
    "#test = test_df.to_numpy()\n",
    "\n",
    "#y_train = train[0]\n",
    "#x_train = train[1]\n",
    "\n",
    "#y_test = test[0]\n",
    "#x_test = test[1]\n",
    "\n",
    "# Binary Labels\n",
    "notes_x = torch.tensor(notes_tok_eff)\n",
    "x_train, x_test, y_train, y_test = train_test_split(notes_x, y, test_size=0.33, random_state=39)\n",
    "    \n",
    "    \n",
    "# Load data to PyTorch DataLoader\n",
    "train_dataloader, val_dataloader = data_loader(x_train, x_test, y_train, y_test, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92bbb3c-2292-4bf7-bc6d-1c6238a11ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 pretrained_embedding=None,\n",
    "                 vocab_size=None,\n",
    "                 embed_dim=300,\n",
    "                 num_classes=2,\n",
    "                 dropout=0.5):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "        # embeddings\n",
    "        self.vocab_size, self.embed_dim = pretrained_embedding.shape\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embedding)\n",
    "        \n",
    "        # Conv Network\n",
    "        self.conv1 =  nn.Conv1d(self.embed_dim,6,kernel_size=5)\n",
    "        self.fc1 = nn.Linear(6, 2)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = F.relu(self.fc1(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1056c5ae-66ca-4232-bf89-9bfdbbe54a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e28741-0c28-4f77-9760-109ecebafcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, n_epoch=n_epochs, optimizer=optimizer, criterion=criterion):\n",
    "    import torch.optim as optim\n",
    "\n",
    "    model.train() # prep model for training\n",
    "    \n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        curr_epoch_loss = []\n",
    "        for data, target in tqdm(train_dataloader):\n",
    "            # your code here\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            \n",
    "            curr_epoch_loss.append(loss.cpu().data.numpy())\n",
    "        print(f\"Epoch {epoch}: curr_epoch_loss={np.mean(curr_epoch_loss)}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07662a2-b54c-490b-be4b-8d84dbdb4eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, dataloader):\n",
    "    model.eval()\n",
    "    Y_pred = []\n",
    "    Y_true = []\n",
    "    for data, target in dataloader:\n",
    "        # your code here\n",
    "        Y_true.append(target)\n",
    "        outputs = model(data)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        Y_pred.append(predicted)\n",
    "    Y_pred = np.concatenate(Y_pred, axis=0)\n",
    "    Y_true = np.concatenate(Y_true, axis=0)\n",
    "\n",
    "    return Y_pred, Y_true"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
