{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb48e8f-1a56-4eaf-afdb-066fbb4151bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pyhealth inflect autocorrect torchtext gensim==3.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d619733d-803c-4882-884b-6fe1d32fe7da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyhealth.datasets import MIMIC4Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4cdc8d-b444-4e13-b1ef-455b4ec33d02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1291cedd-56d1-4549-94e1-f5da8f082ae9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyhealth.datasets import MIMIC4Dataset\n",
    "dataset = MIMIC4Dataset(\n",
    "        root=\"data/mimic4_subset\",\n",
    "        tables=[\"diagnoses_icd\", \"procedures_icd\"],\n",
    "    )\n",
    "#dataset.stat()\n",
    "#dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e72fb8-896a-4bab-a9af-96fadd5b6aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyhealth.medcode import InnerMap\n",
    "\n",
    "icd9cm = InnerMap.load(\"ICD9CM\")\n",
    "icd10cm = InnerMap.load(\"ICD10CM\")\n",
    "#smoker = icd9cm.lookup(\"V15.82\")\n",
    "\n",
    "counter = 0\n",
    "patient_dict = dataset.patients\n",
    "labels = []\n",
    "for subject_id, patient in patient_dict.items():\n",
    "    #if counter > 100:\n",
    "    #    break\n",
    "    #counter += 1\n",
    "    tobacco = 0\n",
    "    visit_dict = patient.visits\n",
    "    for visit_id, visit in visit_dict.items():\n",
    "        #print(visit.encounter_time, visit.available_tables)\n",
    "        events = visit.get_event_list('diagnoses_icd')\n",
    "        for event in events:\n",
    "            if event.vocabulary == 'ICD9CM' and event.code in ['V1582', '3051']:\n",
    "                tobacco = 1\n",
    "                #explain = icd9cm.lookup(event.code)\n",
    "                #print(event.patient_id, event.visit_id, visit.encounter_time, event.vocabulary, event.code, explain)\n",
    "            elif event.vocabulary == 'ICD10CM' and event.code.startswith('F17'):\n",
    "                tobacco = 1\n",
    "                #explain = icd10cm.lookup(event.code)\n",
    "                #print(event.patient_id, event.visit_id, visit.encounter_time, event.vocabulary, event.code, explain)\n",
    "    labels.append({'subject_id':subject_id,'label':tobacco})\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821bce96-924f-46f1-82c3-d73933ea8963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "import inflect\n",
    "from autocorrect import spell\n",
    "from collections import OrderedDict\n",
    "\n",
    "# function that cleans text\n",
    "# still need to account for contractions, abbreviations, and numbers/fractions\n",
    "default_stemmer = PorterStemmer()\n",
    "default_stopwords = stopwords.words('english') # or any other list of your choice\n",
    "def clean_text(text, replace_numbers = False, remove_rare = False, remove_punctuation = False, stem_text = False, remove_stopwords = False, remove_num = False , spell_check = False, remove_repeat = False):\n",
    "        def misc_cleaning(text):\n",
    "                text = re.sub(\"-([a-zA-Z]+)\", r\"\\1\", text) # replaces hyphen with spaces in case of strings\n",
    "                text = re.sub(' y ', '', text) # gets rid of random y accent stuff scattered through the text\n",
    "                text = re.sub('yyy', 'y', text)\n",
    "                text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "                text = re.sub(r\"what's\", \"what is \", text)\n",
    "                text = re.sub(r\"\\'s\", \" \", text)\n",
    "                text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "                text = re.sub(r\"can't\", \"cannot \", text)\n",
    "                text = re.sub(r\"n't\", \" not \", text)\n",
    "                text = re.sub(r\"i'm\", \"i am \", text)\n",
    "                text = re.sub(r\"\\'re\", \" are \", text)\n",
    "                text = re.sub(r\"\\'d\", \" would \", text)\n",
    "                text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "                text = re.sub(r\",\", \" \", text)\n",
    "                text = re.sub(r\"\\.\", \" \", text)\n",
    "                text = re.sub(r\"!\", \" ! \", text)\n",
    "                text = re.sub(r\"\\/\", \" \", text)\n",
    "                text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "                text = re.sub(r\"\\+\", \" + \", text)\n",
    "                text = re.sub(r\"\\-\", \" - \", text)\n",
    "                text = re.sub(r\"\\=\", \" = \", text)\n",
    "                text = re.sub(r\"'\", \" \", text)\n",
    "                text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "                text = re.sub(r\":\", \" : \", text)\n",
    "                text = re.sub(r\" e g \", \" eg \", text)\n",
    "                text = re.sub(r\" b g \", \" bg \", text)\n",
    "                text = re.sub(r\" u s \", \" american \", text)\n",
    "                text = re.sub(r\"\\0s\", \"0\", text)\n",
    "                text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "                text = re.sub(r\"e - mail\", \"email\", text)\n",
    "                text = re.sub(r\"j k\", \"jk\", text)\n",
    "                text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "                return text\n",
    "\n",
    "        # function to tokenize text which is used in a lot of the later processing\n",
    "        def tokenize_text(text):\n",
    "                return [w for s in sent_tokenize(text) for w in word_tokenize(s)]\n",
    "\n",
    "        text = text.strip(' ') # strip whitespaces\n",
    "        text = text.lower() # lowercase\n",
    "        text = misc_cleaning(text) # look at function, random cleaning stuff\n",
    "        \n",
    "        if remove_repeat:\n",
    "                sentences = sent_tokenize(text)\n",
    "                sentences = list(dict.fromkeys(sentences))\n",
    "                text = \" \".join(sentences)\n",
    "        \n",
    "        # removes punctuation\n",
    "        if remove_punctuation:\n",
    "                text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        # optional: replaces numbers (\"3\") with their word counterparts (\"three\")\n",
    "        if replace_numbers:\n",
    "                words = word_tokenize(text)\n",
    "                p = inflect.engine()\n",
    "                new_words = []\n",
    "                for word in words:\n",
    "                        if word.isdigit():\n",
    "                                new_word = p.number_to_words(word)\n",
    "                                new_words.append(new_word)\n",
    "                        else:\n",
    "                                new_words.append(word)\n",
    "                text = \" \".join(new_words)\n",
    "\n",
    "        # optional: removes the rarest words in each text --> right now it's 10\n",
    "        if remove_rare:\n",
    "                tokens = word_tokenize(text)\n",
    "                freq_dist = nltk.FreqDist(tokens)\n",
    "                rarewords = list(freq_dist.keys())[-10:]\n",
    "                new_words = [word for word in tokens if word not in rarewords]\n",
    "                text = \" \".join(new_words)\n",
    "\n",
    "        # optional: stems text using Porter Stemmer\n",
    "        if stem_text:\n",
    "                stemmer = default_stemmer\n",
    "                tokens = tokenize_text(text)\n",
    "                text = \" \".join([stemmer.stem(t) for t in tokens])\n",
    "\n",
    "        # removes stop words such as \"a\", \"the\", etc.\n",
    "        if remove_stopwords:\n",
    "                stop_words = default_stopwords\n",
    "                tokens = [w for w in tokenize_text(text) if w not in stop_words]\n",
    "                text = \" \".join(tokens)\n",
    "        \n",
    "        # optional: removes numbers completely from the ext\n",
    "        if remove_num:\n",
    "                text=text.split()\n",
    "                text=[x for x in text if not x.isnumeric()]\n",
    "                text= \" \".join(text)\n",
    "        \n",
    "        #remove headers from discharge notes\n",
    "        #name unit admission date discharge date date birth sex service medicine allergies known allergies adverse drug reactions attending chief complaint \n",
    "        headers = text.find(\" chief complaint \",1,300)\n",
    "        if headers > -1:\n",
    "            headers += 17\n",
    "            text = text[headers:]\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947d2ec1-217b-4cbd-a21f-f74adab16939",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_notes_discharge = pd.read_csv(\"data/mimic4_notes/discharge.csv\")\n",
    "#df_notes_discharge_detail = pd.read_csv(\"data/mimic4_notes/discharge_detail.csv\")\n",
    "#df_notes_radiology = pd.read_csv(\"data/mimic4_notes/radiology.csv\")\n",
    "\n",
    "print(df_notes_discharge.columns)\n",
    "print('total len', len(df_notes_discharge))\n",
    "#df_notes_discharge_detail.columns\n",
    "\n",
    "#print(df_notes_discharge['subject_id'])\n",
    "#print(df_notes_radiology['text'][0])\n",
    "\n",
    "#group by patient and concatenate all notes for one patient\n",
    "df_notes_discharge = df_notes_discharge.groupby(['subject_id'], as_index = False).agg({'text': ' '.join})\n",
    "print(df_notes_discharge.columns)\n",
    "print('len of patients', len(df_notes_discharge))\n",
    "\n",
    "#limit to 781 patients\n",
    "df_notes_discharge = df_notes_discharge.head(781)\n",
    "print('final len', len(df_notes_discharge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fe066b-7c7d-4e66-a3e0-0f5407bd6505",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.multiprocessing import get\n",
    "\n",
    "#ddf = dd.from_pandas(df_notes_discharge, npartitions=7)\n",
    "#meta_df = pd.DataFrame(columns=[\"subject_id\", \"text\", \"new_text\"], dtype=object)\n",
    "\n",
    "#ddf['text'] = ddf['text'].apply(lambda text: clean_text(text, remove_punctuation = True, remove_stopwords = True, remove_repeat = True))\n",
    "\n",
    "#res = ddf.map_partitions(lambda df: df.assign(new_text = clean_text(df['text'], remove_punctuation = True, remove_stopwords = True, remove_repeat = True)), meta=meta_df)\n",
    "#res.to_csv(\"data/mimic4_notes/discharge_clean.csv\", index=False)\n",
    "\n",
    "#pandas_df = ddf.compute()\n",
    "#pandas_df.to_csv(\"data/mimic4_notes/discharge_clean.csv\", index=False)\n",
    "\n",
    "\n",
    "# without dask\n",
    "df_notes_discharge['text'] = df_notes_discharge['text'].apply(lambda text: clean_text(text, remove_punctuation = True, remove_stopwords = True, remove_repeat = True, remove_num = True))\n",
    "\n",
    "#save notes for embeddings\n",
    "notes = list(df_notes_discharge['text'])     \n",
    "\n",
    "# save cleaned notes into a pickle file\n",
    "f = open('data/cleaned_notes.pckl', 'wb')\n",
    "pickle.dump(notes, f)\n",
    "f.close()\n",
    "print(\"Saved cleansed notes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fd7514f3-5cd8-4bba-bcc0-2282c1e9d817",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training process...\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "# Make Word2Vec embeddings from the notes themselves\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import pickle\n",
    "\n",
    "f = open('data/cleaned_notes.pckl', 'rb')\n",
    "notes = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "def make_w2v_model(notes, window, workers, epochs, vector_size, min_count):\n",
    "    model = gensim.models.Word2Vec(notes, size=vector_size, window=window, min_count=min_count, workers=workers)\n",
    "    print('Start training process...') \n",
    "    model.train(notes,total_examples=len(notes),epochs=epochs)\n",
    "    model.save(\"w2v.model\")\n",
    "    print(\"Model Saved\")\n",
    "\n",
    "make_w2v_model(notes,  window=5, workers=1, epochs=20, vector_size=300, min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b39e2a04-0959-4cce-b616-dce23b1a7543",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load pretrained word embeddings\n",
    "# for lining up with pretrained we need to first \n",
    "# process notes converted to index array of numbers of same length\n",
    "\n",
    "# transforms text to a sequence of integers padded to same length\n",
    "#from keras.preprocessing.text import Tokenizer\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "import numpy as np\n",
    "import statistics\n",
    "# for progress bar\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def textTokenize(notes):\n",
    "    \"\"\"For each patients text, find max length, build a dict of words\n",
    "    \n",
    "    Returns:\n",
    "        tokenized_texts (List[List[str]]): List of list of tokens\n",
    "        word2idx (Dict): Dict built from the corpus\n",
    "        max_len (int): max sentence length\n",
    "    \"\"\"\n",
    "    t = get_tokenizer(\"basic_english\")\n",
    "    lengths = []\n",
    "    tokenized_texts = []\n",
    "    word2idx = {}\n",
    "    # Add padding and unknown tokens to the dictionary\n",
    "    word2idx['<pad>'] = 0\n",
    "    word2idx['<unk>'] = 1\n",
    "    idx = 2\n",
    "    for text in notes:\n",
    "        tokenized_text = t(text)\n",
    "        tokenized_texts.append(tokenized_text)\n",
    "        # Add new token to `word2idx`\n",
    "        for token in tokenized_text:\n",
    "            if token not in word2idx:\n",
    "                word2idx[token] = idx\n",
    "                idx += 1\n",
    "        lengths.append(len(tokenized_text))\n",
    "    mean_len = np.mean(lengths)\n",
    "    std_len = np.std(lengths)\n",
    "    max_len = np.max(lengths)\n",
    "    return tokenized_texts, word2idx, max_len, mean_len, std_len\n",
    "\n",
    "def encodeTokenizedText(tokenized_texts, word2idx, normal_max_len):\n",
    "    \"\"\"Pad each sentence to the max length and encode tokens to their index in the all words dict.\n",
    "    Make it more efficient -  instead of max length, make it mean len + 4x std dev, to eliminate few outliers\n",
    "\n",
    "    Returns:\n",
    "        input_ids (np.array): Array of token indexes in the vocabulary with\n",
    "            shape (N, max_len). It will the input to the CNN.\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    for tokenized_sent in tokenized_texts:\n",
    "        # Pad sentences to normal_max_len\n",
    "        if (len(tokenized_sent) > normal_max_len):\n",
    "            tokenized_sent = tokenized_sent[0:normal_max_len]\n",
    "        else:\n",
    "            tokenized_sent += ['<pad>'] * (normal_max_len - len(tokenized_sent))\n",
    "        if len(tokenized_sent) != normal_max_len:\n",
    "            print(len(tokenized_sent))\n",
    "            print(i)\n",
    "            \n",
    "        # Encode tokens to input_ids\n",
    "        input_id = [word2idx.get(token) for token in tokenized_sent]\n",
    "        input_ids.append(input_id)    \n",
    "    return np.array(input_ids, dtype=int)\n",
    "\n",
    "\n",
    "\n",
    "tokenized_texts, word2idx, max_len, mean_len, std_len = textTokenize(notes)\n",
    "normal_max_len = int((mean_len + 4*std_len) + 1)\n",
    "# input_ids are the input to cnn and rnn models, as the tokenized text\n",
    "input_ids = encodeTokenizedText(tokenized_texts, word2idx, normal_max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "266ae712-0298-4b59-8206-d06a661f63de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "pretrain = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4741adc6-abdb-4067-8204-1579a03bdcc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word vectors len is  37\n",
      "pretrain len is  3000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15828/1982635348.py:35: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  print('pretrain len is ',len(pretrain.wv.vocab))\n",
      "/tmp/ipykernel_15828/1982635348.py:38: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  for i in range(len(pretrain.wv.vocab)):\n",
      "/tmp/ipykernel_15828/1982635348.py:39: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  word=pretrain.wv.index2word[i]\n",
      "/tmp/ipykernel_15828/1982635348.py:41: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  embedding_index[word]=pretrain.wv[word]\n",
      "/tmp/ipykernel_15828/1982635348.py:47: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for word, i in tqdm_notebook(word_index.items()):\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007471323013305664,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 29898,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16026769d47e4c8a8423c3417942ea8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29898 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MAKE EMBEDDING MATRIX\n",
    "\n",
    "def word_Embed_w2v(word_index, model):   \n",
    "    w2v = model\n",
    "    #convert pretrained word embedding to a dictionary\n",
    "    embedding_index=dict()\n",
    "    print('word vectors len is ',len(w2v.wv.vocab))\n",
    "    for i in range(len(w2v.wv.vocab)):\n",
    "        word=w2v.wv.index2word[i]\n",
    "        if word is not None:\n",
    "            embedding_index[word]=w2v.wv[word]  \n",
    "    #extract word embedding for train and test data\n",
    "    \n",
    "    # create matrix of shape\n",
    "    embedding_matrix = np.random.uniform(-0.25, 0.25, (len(word_index), 300))    \n",
    "    embedding_matrix[word_index['<pad>']] = np.zeros((300,))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def word_Embed_GNV(word_index):   \n",
    "    \"\"\" Load the pretrained vectors for each token in our vocabulary. \n",
    "    For tokens with no pretraiend vectors, we will initialize random word vectors with the same length and variance.\n",
    "    \n",
    "     Returns:\n",
    "        embeddings (np.array): Embedding matrix with shape (N, d) where N is\n",
    "            the size of word2idx and d is embedding dimension\n",
    "    \"\"\"\n",
    "    #pretrain = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "    # convert pretrained word embedding to a dictionary\n",
    "    print('pretrain len is ',len(pretrain.wv.vocab))\n",
    "    # fill embedding_index with every word from the pretrain\n",
    "    embedding_index=dict()\n",
    "    for i in range(len(pretrain.wv.vocab)):\n",
    "        word=pretrain.wv.index2word[i]\n",
    "        if word is not None:\n",
    "            embedding_index[word]=pretrain.wv[word] \n",
    "            \n",
    "    # create matrix of shape\n",
    "    embedding_matrix = np.random.uniform(-0.25, 0.25, (len(word_index), 300))    \n",
    "    embedding_matrix[word_index['<pad>']] = np.zeros((300,))\n",
    "    \n",
    "    for word, i in tqdm_notebook(word_index.items()):\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "w2v_model = Word2Vec.load(\"w2v.model\")\n",
    "embedding_matrix_w2v = word_Embed_w2v(word2idx, w2v_model)\n",
    "\n",
    "embedding_matrix_GNV = word_Embed_GNV(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "051fe08f-2838-41d3-b3f6-c96386745d4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Tokenized Notes\n",
      "Saved Google Vector Word Embedding Matrix\n",
      "Saved Word 2 Vector Embedding Matrix\n",
      "Saved Word Indices\n",
      "Saved Maximum Length of One Patient's Notes\n"
     ]
    }
   ],
   "source": [
    "f = open('data/embeddings/tokenized_notes.pckl', 'wb')\n",
    "pickle.dump(input_ids, f)\n",
    "f.close()\n",
    "print(\"Saved Tokenized Notes\")\n",
    "\n",
    "f = open('data/embeddings/embedding_matrix_GNV.pckl', 'wb')\n",
    "pickle.dump(embedding_matrix_GNV, f)\n",
    "f.close()\n",
    "print(\"Saved Google Vector Word Embedding Matrix\")\n",
    "\n",
    "f = open('data/embeddings/embedding_matrix_w2v.pckl', 'wb')\n",
    "pickle.dump(embedding_matrix_w2v, f)\n",
    "f.close()\n",
    "print(\"Saved Word 2 Vector Embedding Matrix\")\n",
    "\n",
    "f = open('data/embeddings/word_index_eff.pckl', 'wb')\n",
    "pickle.dump(word2idx, f)\n",
    "f.close()\n",
    "print(\"Saved Word Indices\")\n",
    "\n",
    "f = open('data/embeddings/max_len_eff.pckl', 'wb')\n",
    "pickle.dump(normal_max_len, f)\n",
    "f.close()\n",
    "print(\"Saved Maximum Length of One Patient's Notes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05ad649-ab57-4cda-aaac-bc3befa85813",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings = torch.tensor(embeddings)\n",
    "\n",
    "#np.random.seed(1234)\n",
    "#from pyhealth.datasets.splitter import split_by_patient\n",
    "#from pyhealth.datasets import split_by_patient, get_dataloader\n",
    "# data split\n",
    "#train_dataset, val_dataset, test_dataset = split_by_patient(dataset, [0.8, 0.1, 0.1])\n",
    "# create dataloaders (they are <torch.data.DataLoader> object)\n",
    "#train_loader = get_dataloader(train_dataset, batch_size=64, shuffle=True)\n",
    "#val_loader = get_dataloader(val_dataset, batch_size=64, shuffle=False)\n",
    "#test_loader = get_dataloader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41360dab-19f5-406d-9ecd-099fa40e5ec5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import (TensorDataset, DataLoader, RandomSampler,\n",
    "                              SequentialSampler)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def data_loader(train_inputs, val_inputs, train_labels, val_labels,\n",
    "                batch_size=50):\n",
    "    \"\"\"Convert train and validation sets to torch.Tensors and load them to\n",
    "    DataLoader.a\n",
    "  a  \"\"\"\n",
    "\n",
    "    # Convert data type to torch.Tensor\n",
    "    train_inputs, val_inputs, train_labels, val_labels =\\\n",
    "    tuple(torch.tensor(data) for data in\n",
    "          [train_inputs, val_inputs, train_labels, val_labels])\n",
    "\n",
    "    # Specify batch_size\n",
    "    batch_size = 50\n",
    "\n",
    "    # Create DataLoader for training data\n",
    "    train_data = TensorDataset(train_inputs, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    # Create DataLoader for validation data\n",
    "    val_data = TensorDataset(val_inputs, val_labels)\n",
    "    val_sampler = SequentialSampler(val_data)\n",
    "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "#df_notes_discharge\n",
    "label_df = pd.DataFrame(labels)\n",
    "\n",
    "label_df[\"subject_id\"] = pd.to_numeric(label_df[\"subject_id\"])\n",
    "# return just the labels of the patients in the correct order as y\n",
    "y = label_df.merge(df_notes_discharge, on='subject_id', how='inner')['label']\n",
    "y = torch.tensor(y.to_numpy())\n",
    "\n",
    "#train_size = int(len(total) * 2/3)\n",
    "#test_size = len(total) - train_size\n",
    "#train_df = total.head(train_size)\n",
    "#test_df = total.tail(test_size)\n",
    "\n",
    "#train = train_df.to_numpy()\n",
    "#test = test_df.to_numpy()\n",
    "\n",
    "#y_train = train[0]\n",
    "#x_train = train[1]\n",
    "\n",
    "#y_test = test[0]\n",
    "#x_test = test[1]\n",
    "\n",
    "\n",
    "\n",
    "# Binary Labels\n",
    "notes_x = torch.tensor(notes_tok_eff)\n",
    "x_train, x_test, y_train, y_test = train_test_split(notes_x, y, test_size=0.33, random_state=39)\n",
    "    \n",
    "    \n",
    "# Load data to PyTorch DataLoader\n",
    "train_dataloader, val_dataloader = data_loader(x_train, x_test, y_train, y_test, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92bbb3c-2292-4bf7-bc6d-1c6238a11ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 pretrained_embedding=None,\n",
    "                 vocab_size=None,\n",
    "                 embed_dim=300,\n",
    "                 num_classes=2,\n",
    "                 dropout=0.5):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "        # embeddings\n",
    "        self.vocab_size, self.embed_dim = pretrained_embedding.shape\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embedding)\n",
    "        \n",
    "        # Conv Network\n",
    "        self.conv1 =  nn.Conv1d(self.embed_dim,6,kernel_size=5)\n",
    "        self.fc1 = nn.Linear(6, 2)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = F.relu(self.fc1(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1056c5ae-66ca-4232-bf89-9bfdbbe54a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e28741-0c28-4f77-9760-109ecebafcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, n_epoch=n_epochs, optimizer=optimizer, criterion=criterion):\n",
    "    import torch.optim as optim\n",
    "\n",
    "    model.train() # prep model for training\n",
    "    \n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        curr_epoch_loss = []\n",
    "        for data, target in tqdm(train_dataloader):\n",
    "            # your code here\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            \n",
    "            curr_epoch_loss.append(loss.cpu().data.numpy())\n",
    "        print(f\"Epoch {epoch}: curr_epoch_loss={np.mean(curr_epoch_loss)}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07662a2-b54c-490b-be4b-8d84dbdb4eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, dataloader):\n",
    "    model.eval()\n",
    "    Y_pred = []\n",
    "    Y_true = []\n",
    "    for data, target in dataloader:\n",
    "        # your code here\n",
    "        Y_true.append(target)\n",
    "        outputs = model(data)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        Y_pred.append(predicted)\n",
    "    Y_pred = np.concatenate(Y_pred, axis=0)\n",
    "    Y_true = np.concatenate(Y_true, axis=0)\n",
    "\n",
    "    return Y_pred, Y_true"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
