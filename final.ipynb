{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cb48e8f-1a56-4eaf-afdb-066fbb4151bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: pyhealth in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (1.1.3)\n",
      "Requirement already satisfied: inflect in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (6.0.4)\n",
      "Requirement already satisfied: autocorrect in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (2.6.1)\n",
      "Requirement already satisfied: torchtext in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (0.14.1)\n",
      "Requirement already satisfied: gensim==3.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from gensim==3.6.0) (1.23.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from gensim==3.6.0) (1.8.1)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from gensim==3.6.0) (5.2.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from gensim==3.6.0) (1.16.0)\n",
      "Requirement already satisfied: networkx>=2.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pyhealth) (3.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pyhealth) (4.63.2)\n",
      "Requirement already satisfied: torch>=1.8.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pyhealth) (1.13.1)\n",
      "Requirement already satisfied: rdkit>=2022.03.4 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pyhealth) (2022.9.5)\n",
      "Requirement already satisfied: pandas>=1.3.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pyhealth) (1.4.4)\n",
      "Requirement already satisfied: scikit-learn>=0.24.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pyhealth) (1.0)\n",
      "Requirement already satisfied: pydantic>=1.9.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from inflect) (1.10.4)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from torchtext) (2.28.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pandas>=1.3.2->pyhealth) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pandas>=1.3.2->pyhealth) (2022.7)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pydantic>=1.9.1->inflect) (4.4.0)\n",
      "Requirement already satisfied: Pillow in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from rdkit>=2022.03.4->pyhealth) (9.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from scikit-learn>=0.24.2->pyhealth) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from scikit-learn>=0.24.2->pyhealth) (3.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->torchtext) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->torchtext) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->torchtext) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->torchtext) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyhealth inflect autocorrect torchtext gensim==3.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf4cdc8d-b444-4e13-b1ef-455b4ec33d02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyhealth.medcode import InnerMap\n",
    "from pyhealth.datasets import MIMIC4Dataset\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "import inflect\n",
    "from autocorrect import spell\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "import numpy as np\n",
    "import statistics\n",
    "# for progress bar\n",
    "from tqdm import tqdm_notebook\n",
    "import random\n",
    "import json\n",
    "import tqdm\n",
    "from sklearn.metrics import *\n",
    "\n",
    "# set seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1291cedd-56d1-4549-94e1-f5da8f082ae9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = MIMIC4Dataset(\n",
    "        root=\"data/mimic4_subset\",\n",
    "        tables=[\"diagnoses_icd\", \"procedures_icd\"],\n",
    "    )\n",
    "#dataset.stat()\n",
    "#dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16e72fb8-896a-4bab-a9af-96fadd5b6aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "icd9cm = InnerMap.load(\"ICD9CM\")\n",
    "icd10cm = InnerMap.load(\"ICD10CM\")\n",
    "#smoker = icd9cm.lookup(\"V15.82\")\n",
    "\n",
    "counter = 0\n",
    "patient_dict = dataset.patients\n",
    "labels = []\n",
    "for subject_id, patient in patient_dict.items():\n",
    "    #if counter > 100:\n",
    "    #    break\n",
    "    #counter += 1\n",
    "    tobacco = 0\n",
    "    visit_dict = patient.visits\n",
    "    for visit_id, visit in visit_dict.items():\n",
    "        #print(visit.encounter_time, visit.available_tables)\n",
    "        events = visit.get_event_list('diagnoses_icd')\n",
    "        for event in events:\n",
    "            if event.vocabulary == 'ICD9CM' and event.code in ['V1582', '3051']:\n",
    "                tobacco = 1\n",
    "                #explain = icd9cm.lookup(event.code)\n",
    "                #print(event.patient_id, event.visit_id, visit.encounter_time, event.vocabulary, event.code, explain)\n",
    "            elif event.vocabulary == 'ICD10CM' and event.code.startswith('F17'):\n",
    "                tobacco = 1\n",
    "                #explain = icd10cm.lookup(event.code)\n",
    "                #print(event.patient_id, event.visit_id, visit.encounter_time, event.vocabulary, event.code, explain)\n",
    "    labels.append({'subject_id':subject_id,'label':tobacco})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "821bce96-924f-46f1-82c3-d73933ea8963",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function that cleans text\n",
    "# still need to account for contractions, abbreviations, and numbers/fractions\n",
    "default_stemmer = PorterStemmer()\n",
    "default_stopwords = stopwords.words('english') # or any other list of your choice\n",
    "def clean_text(text, replace_numbers = False, remove_rare = False, remove_punctuation = False, stem_text = False, remove_stopwords = False, remove_num = False , spell_check = False, remove_repeat = False):\n",
    "        def misc_cleaning(text):\n",
    "                text = re.sub(\"-([a-zA-Z]+)\", r\"\\1\", text) # replaces hyphen with spaces in case of strings\n",
    "                text = re.sub(' y ', '', text) # gets rid of random y accent stuff scattered through the text\n",
    "                text = re.sub('yyy', 'y', text)\n",
    "                text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "                text = re.sub(r\"what's\", \"what is \", text)\n",
    "                text = re.sub(r\"\\'s\", \" \", text)\n",
    "                text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "                text = re.sub(r\"can't\", \"cannot \", text)\n",
    "                text = re.sub(r\"n't\", \" not \", text)\n",
    "                text = re.sub(r\"i'm\", \"i am \", text)\n",
    "                text = re.sub(r\"\\'re\", \" are \", text)\n",
    "                text = re.sub(r\"\\'d\", \" would \", text)\n",
    "                text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "                text = re.sub(r\",\", \" \", text)\n",
    "                text = re.sub(r\"\\.\", \" \", text)\n",
    "                text = re.sub(r\"!\", \" ! \", text)\n",
    "                text = re.sub(r\"\\/\", \" \", text)\n",
    "                text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "                text = re.sub(r\"\\+\", \" + \", text)\n",
    "                text = re.sub(r\"\\-\", \" - \", text)\n",
    "                text = re.sub(r\"\\=\", \" = \", text)\n",
    "                text = re.sub(r\"'\", \" \", text)\n",
    "                text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "                text = re.sub(r\":\", \" : \", text)\n",
    "                text = re.sub(r\" e g \", \" eg \", text)\n",
    "                text = re.sub(r\" b g \", \" bg \", text)\n",
    "                text = re.sub(r\" u s \", \" american \", text)\n",
    "                text = re.sub(r\"\\0s\", \"0\", text)\n",
    "                text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "                text = re.sub(r\"e - mail\", \"email\", text)\n",
    "                text = re.sub(r\"j k\", \"jk\", text)\n",
    "                text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "                return text\n",
    "\n",
    "        # function to tokenize text which is used in a lot of the later processing\n",
    "        def tokenize_text(text):\n",
    "                return [w for s in sent_tokenize(text) for w in word_tokenize(s)]\n",
    "\n",
    "        text = text.strip(' ') # strip whitespaces\n",
    "        text = text.lower() # lowercase\n",
    "        text = misc_cleaning(text) # look at function, random cleaning stuff\n",
    "        \n",
    "        if remove_repeat:\n",
    "                sentences = sent_tokenize(text)\n",
    "                sentences = list(dict.fromkeys(sentences))\n",
    "                text = \" \".join(sentences)\n",
    "        \n",
    "        # removes punctuation\n",
    "        if remove_punctuation:\n",
    "                text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        # optional: replaces numbers (\"3\") with their word counterparts (\"three\")\n",
    "        if replace_numbers:\n",
    "                words = word_tokenize(text)\n",
    "                p = inflect.engine()\n",
    "                new_words = []\n",
    "                for word in words:\n",
    "                        if word.isdigit():\n",
    "                                new_word = p.number_to_words(word)\n",
    "                                new_words.append(new_word)\n",
    "                        else:\n",
    "                                new_words.append(word)\n",
    "                text = \" \".join(new_words)\n",
    "\n",
    "        # optional: removes the rarest words in each text --> right now it's 10\n",
    "        if remove_rare:\n",
    "                tokens = word_tokenize(text)\n",
    "                freq_dist = nltk.FreqDist(tokens)\n",
    "                rarewords = list(freq_dist.keys())[-10:]\n",
    "                new_words = [word for word in tokens if word not in rarewords]\n",
    "                text = \" \".join(new_words)\n",
    "\n",
    "        # optional: stems text using Porter Stemmer\n",
    "        if stem_text:\n",
    "                stemmer = default_stemmer\n",
    "                tokens = tokenize_text(text)\n",
    "                text = \" \".join([stemmer.stem(t) for t in tokens])\n",
    "\n",
    "        # removes stop words such as \"a\", \"the\", etc.\n",
    "        if remove_stopwords:\n",
    "                stop_words = default_stopwords\n",
    "                tokens = [w for w in tokenize_text(text) if w not in stop_words]\n",
    "                text = \" \".join(tokens)\n",
    "        \n",
    "        # optional: removes numbers completely from the ext\n",
    "        if remove_num:\n",
    "                text=text.split()\n",
    "                text=[x for x in text if not x.isnumeric()]\n",
    "                text= \" \".join(text)\n",
    "        \n",
    "        #remove headers from discharge notes\n",
    "        #name unit admission date discharge date date birth sex service medicine allergies known allergies adverse drug reactions attending chief complaint \n",
    "        headers = text.find(\" chief complaint \",1,300)\n",
    "        if headers > -1:\n",
    "            headers += 17\n",
    "            text = text[headers:]\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "947d2ec1-217b-4cbd-a21f-f74adab16939",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['note_id', 'subject_id', 'hadm_id', 'note_type', 'note_seq',\n",
      "       'charttime', 'storetime', 'text'],\n",
      "      dtype='object')\n",
      "total len 331794\n",
      "Index(['subject_id', 'text'], dtype='object')\n",
      "len of patients 145915\n",
      "final len 781\n"
     ]
    }
   ],
   "source": [
    "df_notes_discharge = pd.read_csv(\"data/mimic4_notes/discharge.csv\")\n",
    "#df_notes_discharge_detail = pd.read_csv(\"data/mimic4_notes/discharge_detail.csv\")\n",
    "#df_notes_radiology = pd.read_csv(\"data/mimic4_notes/radiology.csv\")\n",
    "\n",
    "print(df_notes_discharge.columns)\n",
    "print('total len', len(df_notes_discharge))\n",
    "#df_notes_discharge_detail.columns\n",
    "\n",
    "#print(df_notes_discharge['subject_id'])\n",
    "#print(df_notes_radiology['text'][0])\n",
    "\n",
    "#group by patient and concatenate all notes for one patient\n",
    "df_notes_discharge = df_notes_discharge.groupby(['subject_id'], as_index = False).agg({'text': ' '.join})\n",
    "print(df_notes_discharge.columns)\n",
    "print('len of patients', len(df_notes_discharge))\n",
    "\n",
    "#limit to 781 patients\n",
    "df_notes_discharge = df_notes_discharge.head(781)\n",
    "print('final len', len(df_notes_discharge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11fe066b-7c7d-4e66-a3e0-0f5407bd6505",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleansed notes\n",
      "Saved cleansed df_notes_discharge \n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.multiprocessing import get\n",
    "\n",
    "#ddf = dd.from_pandas(df_notes_discharge, npartitions=7)\n",
    "#meta_df = pd.DataFrame(columns=[\"subject_id\", \"text\", \"new_text\"], dtype=object)\n",
    "\n",
    "#ddf['text'] = ddf['text'].apply(lambda text: clean_text(text, remove_punctuation = True, remove_stopwords = True, remove_repeat = True))\n",
    "\n",
    "#res = ddf.map_partitions(lambda df: df.assign(new_text = clean_text(df['text'], remove_punctuation = True, remove_stopwords = True, remove_repeat = True)), meta=meta_df)\n",
    "#res.to_csv(\"data/mimic4_notes/discharge_clean.csv\", index=False)\n",
    "\n",
    "#pandas_df = ddf.compute()\n",
    "#pandas_df.to_csv(\"data/mimic4_notes/discharge_clean.csv\", index=False)\n",
    "\n",
    "\n",
    "# without dask\n",
    "df_notes_discharge['text'] = df_notes_discharge['text'].apply(lambda text: clean_text(text, remove_punctuation = True, remove_stopwords = True, remove_repeat = True, remove_num = True))\n",
    "\n",
    "\n",
    "#save notes for embeddings\n",
    "notes = list(df_notes_discharge['text'])     \n",
    "\n",
    "# save cleaned notes into a pickle file\n",
    "f = open('data/cleaned_notes.pckl', 'wb')\n",
    "pickle.dump(notes, f)\n",
    "f.close()\n",
    "print(\"Saved cleansed notes\")\n",
    "\n",
    "f = open('data/df_notes_discharge.pckl', 'wb')\n",
    "pickle.dump(df_notes_discharge, f)\n",
    "f.close()\n",
    "print(\"Saved cleansed df_notes_discharge \")\n",
    "\n",
    "# save labels of same size and order\n",
    "\n",
    "label_df = pd.DataFrame(labels)\n",
    "\n",
    "label_df[\"subject_id\"] = pd.to_numeric(label_df[\"subject_id\"])\n",
    "# return just the labels of the patients in the correct order as y, order of left table is maintained.\n",
    "labels = df_notes_discharge.merge(label_df, on='subject_id', how='inner')['label']\n",
    "labels = labels.to_numpy()\n",
    "\n",
    "with open(\"data/labels.pckl\", \"wb\") as f:\n",
    "    pickle.dump(labels, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b39e2a04-0959-4cce-b616-dce23b1a7543",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# process notes converted to index array of numbers of same length\n",
    "\n",
    "# transforms text to a sequence of integers padded to same length\n",
    "#from keras.preprocessing.text import Tokenizer\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def textTokenize(notes):\n",
    "    \"\"\"For each patients text, find max length, build a dict of words\n",
    "    \n",
    "    Returns:\n",
    "        tokenized_texts (List[List[str]]): List of list of tokens\n",
    "        word2idx (Dict): Dict built from the corpus\n",
    "        max_len (int): max sentence length\n",
    "    \"\"\"\n",
    "    t = get_tokenizer(\"basic_english\")\n",
    "    lengths = []\n",
    "    tokenized_texts = []\n",
    "    word2idx = {}\n",
    "    # Add padding and unknown tokens to the dictionary\n",
    "    word2idx['<pad>'] = 0\n",
    "    word2idx['<unk>'] = 1\n",
    "    idx = 2\n",
    "    for text in notes:\n",
    "        tokenized_text = t(text)\n",
    "        tokenized_texts.append(tokenized_text)\n",
    "        # Add new token to `word2idx`\n",
    "        for token in tokenized_text:\n",
    "            if token not in word2idx:\n",
    "                word2idx[token] = idx\n",
    "                idx += 1\n",
    "        lengths.append(len(tokenized_text))\n",
    "    mean_len = np.mean(lengths)\n",
    "    std_len = np.std(lengths)\n",
    "    max_len = np.max(lengths)\n",
    "    return tokenized_texts, word2idx, max_len, mean_len, std_len\n",
    "\n",
    "def encodeTokenizedText(tokenized_texts, word2idx, normal_max_len):\n",
    "    \"\"\"Pad each sentence to the max length and encode tokens to their index in the all words dict.\n",
    "    Make it more efficient -  instead of max length, make it mean len + 4x std dev, to eliminate few outliers\n",
    "\n",
    "    Returns:\n",
    "        input_ids (np.array): Array of token indexes in the vocabulary with\n",
    "            shape (N, max_len). It will the input to the CNN.\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    for tokenized_sent in tokenized_texts:\n",
    "        # Pad sentences to normal_max_len\n",
    "        if (len(tokenized_sent) > normal_max_len):\n",
    "            tokenized_sent = tokenized_sent[0:normal_max_len]\n",
    "        else:\n",
    "            tokenized_sent += ['<pad>'] * (normal_max_len - len(tokenized_sent))\n",
    "        if len(tokenized_sent) != normal_max_len:\n",
    "            print(len(tokenized_sent))\n",
    "            print(i)\n",
    "            \n",
    "        # Encode tokens to input_ids, input_id is just the idx position when it was inserted, so it converts words to numbers\n",
    "        input_id = [word2idx.get(token) for token in tokenized_sent]\n",
    "        input_ids.append(input_id)    \n",
    "    return np.array(input_ids, dtype=int)\n",
    "\n",
    "\n",
    "\n",
    "tokenized_texts, word2idx, max_len, mean_len, std_len = textTokenize(notes)\n",
    "normal_max_len = int((mean_len + 4*std_len) + 1)\n",
    "\n",
    "\n",
    "# input_ids are the input to cnn and rnn models, as the tokenized text\n",
    "input_ids = encodeTokenizedText(tokenized_texts, word2idx, normal_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd7514f3-5cd8-4bba-bcc0-2282c1e9d817",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "781"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4741adc6-abdb-4067-8204-1579a03bdcc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training process...\n",
      "Model Saved\n",
      "word vectors len is  37\n",
      "pretrain len is  3000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21101/2652979663.py:60: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  print('pretrain len is ',len(pretrain.wv.vocab))\n",
      "/tmp/ipykernel_21101/2652979663.py:63: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  for i in range(len(pretrain.wv.vocab)):\n",
      "/tmp/ipykernel_21101/2652979663.py:64: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  word=pretrain.wv.index2word[i]\n",
      "/tmp/ipykernel_21101/2652979663.py:66: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  embedding_index[word]=pretrain.wv[word]\n",
      "/tmp/ipykernel_21101/2652979663.py:72: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for word, i in tqdm_notebook(word_index.items()):\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006216764450073242,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 29898,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f2c4a6e857949d3a2edf1a722196461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29898 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MAKE EMBEDDING MATRIX\n",
    "import gensim.downloader as api\n",
    "\n",
    "#pretrain = api.load('word2vec-google-news-300')\n",
    "\n",
    "#f = open('data/embeddings/pretrain.pckl', 'wb')\n",
    "#pickle.dump(pretrain, f)\n",
    "#f.close()\n",
    "#print(\"Saved pretrain\")\n",
    "\n",
    "with open('data/embeddings/pretrain.pckl', 'rb') as f:\n",
    "    pretrain = pickle.load(f)\n",
    "\n",
    "# Make Word2Vec embeddings from the notes themselves\n",
    "f = open('data/cleaned_notes.pckl', 'rb')\n",
    "notes = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "def make_w2v_model(notes, window, workers, epochs, vector_size, min_count):\n",
    "    model = gensim.models.Word2Vec(notes, size=vector_size, window=window, min_count=min_count, workers=workers)\n",
    "    print('Start training process...') \n",
    "    model.train(notes,total_examples=len(notes),epochs=epochs)\n",
    "    model.save(\"w2v.model\")\n",
    "    print(\"Model Saved\")\n",
    "\n",
    "make_w2v_model(notes,  window=5, workers=1, epochs=20, vector_size=300, min_count=2)\n",
    "\n",
    "def word_Embed_w2v(word_index, model):   \n",
    "    w2v = model\n",
    "    #convert pretrained word embedding to a dictionary\n",
    "    embedding_index=dict()\n",
    "    print('word vectors len is ',len(w2v.wv.vocab))\n",
    "    for i in range(len(w2v.wv.vocab)):\n",
    "        word=w2v.wv.index2word[i]\n",
    "        if word is not None:\n",
    "            embedding_index[word]=w2v.wv[word]  \n",
    "    #extract word embedding for train and test data\n",
    "    \n",
    "    # create matrix of shape\n",
    "    embedding_matrix = np.random.uniform(-0.25, 0.25, (len(word_index), 300))    \n",
    "    embedding_matrix[word_index['<pad>']] = np.zeros((300,))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def word_Embed_GNV(word_index):   \n",
    "    \"\"\" Load the pretrained vectors for each token in our vocabulary. \n",
    "    For tokens with no pretraiend vectors, we will initialize random word vectors with the same length and variance.\n",
    "    \n",
    "     Returns:\n",
    "        embeddings (np.array): Embedding matrix with shape (N, d) where N is\n",
    "            the size of word2idx and d is embedding dimension\n",
    "    \"\"\"\n",
    "    #pretrain = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "    # convert pretrained word embedding to a dictionary\n",
    "    print('pretrain len is ',len(pretrain.wv.vocab))\n",
    "    # fill embedding_index with every word from the pretrain\n",
    "    embedding_index=dict()\n",
    "    for i in range(len(pretrain.wv.vocab)):\n",
    "        word=pretrain.wv.index2word[i]\n",
    "        if word is not None:\n",
    "            embedding_index[word]=pretrain.wv[word] \n",
    "            \n",
    "    # create matrix of shape\n",
    "    embedding_matrix = np.random.uniform(-0.25, 0.25, (len(word_index), 300))    \n",
    "    embedding_matrix[word_index['<pad>']] = np.zeros((300,))\n",
    "    \n",
    "    for word, i in tqdm_notebook(word_index.items()):\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "w2v_model = Word2Vec.load(\"w2v.model\")\n",
    "embedding_matrix_w2v = word_Embed_w2v(word2idx, w2v_model)\n",
    "\n",
    "embedding_matrix_GNV = word_Embed_GNV(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "051fe08f-2838-41d3-b3f6-c96386745d4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Tokenized Notes\n",
      "Saved Google Vector Word Embedding Matrix\n",
      "Saved Word 2 Vector Embedding Matrix\n",
      "Saved Word Indices\n",
      "Saved Maximum Length of One Patient's Notes\n"
     ]
    }
   ],
   "source": [
    "#dump encoded notes and embeddings\n",
    "\n",
    "f = open('data/embeddings/tokenized_notes.pckl', 'wb')\n",
    "pickle.dump(input_ids, f)\n",
    "f.close()\n",
    "print(\"Saved Tokenized Notes\")\n",
    "\n",
    "f = open('data/embeddings/embedding_matrix_GNV.pckl', 'wb')\n",
    "pickle.dump(embedding_matrix_GNV, f)\n",
    "f.close()\n",
    "print(\"Saved Google Vector Word Embedding Matrix\")\n",
    "\n",
    "f = open('data/embeddings/embedding_matrix_w2v.pckl', 'wb')\n",
    "pickle.dump(embedding_matrix_w2v, f)\n",
    "f.close()\n",
    "print(\"Saved Word 2 Vector Embedding Matrix\")\n",
    "\n",
    "f = open('data/embeddings/word_index_eff.pckl', 'wb')\n",
    "pickle.dump(word2idx, f)\n",
    "f.close()\n",
    "print(\"Saved Word Indices\")\n",
    "\n",
    "f = open('data/embeddings/max_len_eff.pckl', 'wb')\n",
    "pickle.dump(normal_max_len, f)\n",
    "f.close()\n",
    "print(\"Saved Maximum Length of One Patient's Notes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9385c47b-917d-4c12-813b-43bb635a7e78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18a8297-a8a3-439d-ba54-f27c403417ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b54d7aed-2929-4636-83ef-a07e3fc2da0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('data/embeddings/tokenized_notes.pckl', 'rb') as f:\n",
    "    input_ids = pickle.load(f)\n",
    "\n",
    "with open('data/embeddings/embedding_matrix_GNV.pckl', 'rb') as f:\n",
    "    embedding_matrix_GNV = pickle.load(f)\n",
    "    embedding_matrix_GNV = torch.tensor(embedding_matrix_GNV)\n",
    "\n",
    "with open('data/embeddings/embedding_matrix_w2v.pckl', 'rb') as f:\n",
    "    embedding_matrix_w2v = pickle.load(f)\n",
    "    embedding_matrix_w2v = torch.tensor(embedding_matrix_w2v)\n",
    "\n",
    "with open('data/embeddings/word_index_eff.pckl', 'rb') as f:\n",
    "    word2idx = pickle.load(f)\n",
    "\n",
    "with open('data/embeddings/max_len_eff.pckl', 'rb') as f:\n",
    "    normal_max_len = pickle.load(f)\n",
    "\n",
    "with open('data/labels.pckl', 'rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "\n",
    "with open('data/df_notes_discharge.pckl', 'rb') as f:\n",
    "    df_notes_discharge = pickle.load(f)\n",
    "    \n",
    "#with open('data/embeddings/pretrain.pckl', 'rb')\n",
    "#    pretrain = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ccb3fa3-6af6-4902-b3ac-64fb2ce42636",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained GNV num embeddings  29898\n",
      "GNV embedding dimensions  300\n",
      "pretrained w2v num embeddings  29898\n",
      "w2v embedding dimensions  300\n",
      "len encoded notes, or total notes is  781\n",
      "len of first note is  16410\n",
      "max len is  16410\n",
      "len or word index, or total unique words is  29898\n"
     ]
    }
   ],
   "source": [
    "print('pretrained GNV num embeddings ', len(embedding_matrix_GNV))\n",
    "print('GNV embedding dimensions ', len(embedding_matrix_GNV[0]))\n",
    "\n",
    "print('pretrained w2v num embeddings ', len(embedding_matrix_w2v))\n",
    "print('w2v embedding dimensions ', len(embedding_matrix_w2v[0]))\n",
    "\n",
    "print('len encoded notes, or total notes is ', len(input_ids))\n",
    "print('len of first note is ', len(input_ids[0]))\n",
    "print('max len is ', normal_max_len)\n",
    "print('len or word index, or total unique words is ', len(word2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a05ad649-ab57-4cda-aaac-bc3befa85813",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings = torch.tensor(embeddings)\n",
    "\n",
    "#from pyhealth.datasets.splitter import split_by_patient\n",
    "#from pyhealth.datasets import split_by_patient, get_dataloader\n",
    "# data split\n",
    "#train_dataset, val_dataset, test_dataset = split_by_patient(dataset, [0.8, 0.1, 0.1])\n",
    "# create dataloaders (they are <torch.data.DataLoader> object)\n",
    "#train_loader = get_dataloader(train_dataset, batch_size=64, shuffle=True)\n",
    "#val_loader = get_dataloader(val_dataset, batch_size=64, shuffle=False)\n",
    "#test_loader = get_dataloader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41360dab-19f5-406d-9ecd-099fa40e5ec5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import (TensorDataset, DataLoader, RandomSampler, SequentialSampler)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def data_loader(x_train, x_test, y_train, y_test, batch_size=8):\n",
    "    \"\"\"Convert train and test sets to tensors and load them to a dataLoader\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert data type to torch.Tensor\n",
    "    x_train, x_test, y_train, y_test = tuple(torch.tensor(data) for data in [x_train, x_test, y_train, y_test])\n",
    "\n",
    "    # Create DataLoader for training data\n",
    "    train_data = TensorDataset(x_train, y_train)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    # Create DataLoader for validation data\n",
    "    val_data = TensorDataset(x_test, y_test)\n",
    "    val_sampler = SequentialSampler(val_data)\n",
    "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "\n",
    "#train_size = int(len(total) * 2/3)\n",
    "#test_size = len(total) - train_size\n",
    "#train_df = total.head(train_size)\n",
    "#test_df = total.tail(test_size)\n",
    "\n",
    "#train = train_df.to_numpy()\n",
    "#test = test_df.to_numpy()\n",
    "\n",
    "#y_train = train[0]\n",
    "#x_train = train[1]\n",
    "\n",
    "#y_test = test[0]\n",
    "#x_test = test[1]\n",
    "\n",
    "\n",
    "    \n",
    " # Train Test Split\n",
    "x_train, x_test, y_train, y_test = train_test_split(input_ids, labels, test_size=0.33, random_state=seed)\n",
    "\n",
    "batch_size=8\n",
    "# Load data to PyTorch DataLoader\n",
    "train_dataloader, val_dataloader = data_loader(x_train, x_test, y_train, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a92bbb3c-2292-4bf7-bc6d-1c6238a11ffe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 pretrained_embedding=None,\n",
    "                 freeze_embedding=False,\n",
    "                 vocab_size=None,\n",
    "                 embed_dim=300,\n",
    "                 num_classes=2,\n",
    "                 dropout=0.1):\n",
    "        \n",
    "        super(CNN, self).__init__()\n",
    "                \n",
    "        # embeddings\n",
    "        if pretrained_embedding is not None:\n",
    "            self.vocab_size, self.embed_dim = pretrained_embedding.shape\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_embedding, freeze=freeze_embedding)\n",
    "        else:\n",
    "            self.embed_dim = embed_dim\n",
    "            self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=self.embed_dim, padding_idx=0)\n",
    "            \n",
    "        self.filters_out = 128\n",
    "        # Conv Network\n",
    "        self.conv1 =  nn.Conv1d(self.embed_dim, self.filters_out, kernel_size=5)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc1 = nn.Linear(self.filters_out,10)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        #output layer\n",
    "        if num_classes == 2:\n",
    "            #binary\n",
    "            self.fc2 = nn.Linear(10, 1)\n",
    "            self.out = nn.Sigmoid()\n",
    "        else:\n",
    "            self.fc2 = nn.Linear(10, num_classes)\n",
    "            self.out = nn.Softmax()         \n",
    "                \n",
    "        nn.init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.fc2.weight, nonlinearity='relu')\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \n",
    "        # get embeddings. Output shape: (b, max_len, embed_dim)\n",
    "        x = self.embedding(input_ids).float()\n",
    "        #print(x)\n",
    "        # Permute to match input shape requirement of nn.Conv1d. Output shape: (b, embed_dim, max_len)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "\n",
    "        #Max pooling. Output shape: (b, self.filters_out, 1)\n",
    "        #input is a,b,in output is a,b,out\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Output shape: (b, self.filters_out)\n",
    "        x = x.squeeze()\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        # final output activation function\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1056c5ae-66ca-4232-bf89-9bfdbbe54a0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = CNN(pretrained_embedding=embedding_matrix_GNV)\n",
    "criterion = torch.nn.BCELoss()\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), weight_decay=0.0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c07662a2-b54c-490b-be4b-8d84dbdb4eba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_model(model, dataloader):\n",
    "    model.eval()\n",
    "    Y_pred = []\n",
    "    Y_true = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            # your code here\n",
    "            Y_true.append(target)\n",
    "            Y_scores = model(data).squeeze()\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            predicted = (Y_scores > .5).int()\n",
    "            Y_pred.append(predicted)\n",
    "        Y_pred = np.concatenate(Y_pred, axis=0)\n",
    "        Y_true = np.concatenate(Y_true, axis=0)\n",
    "        #f1 precision recall accuracy\n",
    "        p, r, f, _ = precision_recall_fscore_support(Y_true, Y_pred, average='weighted')\n",
    "        a = accuracy_score(Y_true, Y_pred)\n",
    "    \n",
    "    return p, r, f, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56e28741-0c28-4f77-9760-109ecebafcce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "def train_model(model, train_dataloader, n_epoch=n_epochs, optimizer=optimizer, criterion=criterion):\n",
    "\n",
    "    model.train() # prep model for training\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        curr_epoch_loss = []\n",
    "        #count = 0\n",
    "        for data, target in train_dataloader:\n",
    "            #if count > 1:\n",
    "            #    break\n",
    "            #count = 1\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(data)\n",
    "            outputs = outputs.squeeze()\n",
    "            loss = criterion(outputs, target.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            curr_epoch_loss.append(loss.cpu().data.numpy())\n",
    "        print(f\"Epoch {epoch + 1}: curr_epoch_loss={np.mean(curr_epoch_loss)}\")\n",
    "        p, r, f, a = eval_model(model, val_dataloader)\n",
    "        print('Epoch: {} \\t Validation precision: {:.2f}, recall:{:.2f}, f1_score: {:.2f}, accuracy: {:.2f}'.format(epoch + 1, p, r, f, a))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4bd64ceb-c796-445b-b223-317bc5bd1eaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: curr_epoch_loss=0.6217027306556702\n",
      "Epoch: 1 \t Validation precision: 0.68, recall:0.65, f1_score: 0.66, accuracy: 0.65\n",
      "Epoch 2: curr_epoch_loss=0.4941687285900116\n",
      "Epoch: 2 \t Validation precision: 0.69, recall:0.65, f1_score: 0.66, accuracy: 0.65\n",
      "Epoch 3: curr_epoch_loss=0.3355555534362793\n",
      "Epoch: 3 \t Validation precision: 0.81, recall:0.74, f1_score: 0.63, accuracy: 0.74\n",
      "Epoch 4: curr_epoch_loss=0.17250755429267883\n",
      "Epoch: 4 \t Validation precision: 0.70, recall:0.71, f1_score: 0.70, accuracy: 0.71\n",
      "Epoch 5: curr_epoch_loss=0.06368406862020493\n",
      "Epoch: 5 \t Validation precision: 0.72, recall:0.71, f1_score: 0.71, accuracy: 0.71\n",
      "Epoch 6: curr_epoch_loss=0.01895950548350811\n",
      "Epoch: 6 \t Validation precision: 0.75, recall:0.77, f1_score: 0.75, accuracy: 0.77\n",
      "Epoch 7: curr_epoch_loss=0.006719441153109074\n",
      "Epoch: 7 \t Validation precision: 0.75, recall:0.77, f1_score: 0.74, accuracy: 0.77\n",
      "Epoch 8: curr_epoch_loss=0.004408594686537981\n",
      "Epoch: 8 \t Validation precision: 0.75, recall:0.77, f1_score: 0.74, accuracy: 0.77\n",
      "Epoch 9: curr_epoch_loss=0.003315099747851491\n",
      "Epoch: 9 \t Validation precision: 0.76, recall:0.78, f1_score: 0.75, accuracy: 0.78\n",
      "Epoch 10: curr_epoch_loss=0.002589722629636526\n",
      "Epoch: 10 \t Validation precision: 0.76, recall:0.78, f1_score: 0.75, accuracy: 0.78\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c49b9e8d-308e-439b-ad5f-d1e0e761c624",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 \t Validation precision: 0.76, recall:0.78, f1_score: 0.75, accuracy: 0.78\n"
     ]
    }
   ],
   "source": [
    "p, r, f, a = eval_model(model, val_dataloader)\n",
    "print('Epoch: {} \\t Validation precision: {:.2f}, recall:{:.2f}, f1_score: {:.2f}, accuracy: {:.2f}'.format(n_epochs, p, r, f, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36027d3b-33df-476e-973e-7765312fb822",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________\n",
      "Classifier: Logistic Regression\n",
      "Accuracy: 0.7596899224806202\n",
      "Precision: 0.5777777777777777\n",
      "Recall: 0.37681159420289856\n",
      "F1-score: 0.45614035087719296\n",
      "______________________________________________\n",
      "\n",
      "______________________________________________\n",
      "Classifier: SVM\n",
      "Accuracy: 0.7596899224806202\n",
      "Precision: 0.6521739130434783\n",
      "Recall: 0.21739130434782608\n",
      "F1-score: 0.32608695652173914\n",
      "______________________________________________\n",
      "\n",
      "______________________________________________\n",
      "Classifier: Naive Bayes\n",
      "Accuracy: 0.7325581395348837\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1-score: 0.0\n",
      "______________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Classic machine learning\n",
    "\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer # add reference\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "RANDOM_STATE = seed\n",
    "\n",
    "def logistic_regression_pred(X_train, Y_train, X_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    logistic regression classifier using X_train and Y_train to predict labels of X_train\n",
    "    \"\"\"\n",
    "    #clf = LogisticRegression(random_state=RANDOM_STATE).fit(X_train, Y_train)\n",
    "    #Y_pred = clf.predict(X_train)\n",
    "    #return Y_pred\n",
    "    logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "               ])\n",
    "    logreg.fit(X_train, Y_train)\n",
    "    Y_pred = logreg.predict(X_test)\n",
    "    return Y_pred\n",
    "\n",
    "def svm_pred(X_train, Y_train, X_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    SVM classifier using X_train and Y_train to predict labels of X_train\n",
    "    \"\"\"\n",
    "    #clf = LinearSVC(random_state=RANDOM_STATE).fit(X_train, Y_train)\n",
    "    #Y_pred = clf.predict(X_train)\n",
    "    #return Y_pred\n",
    "    sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "                ])\n",
    "    sgd.fit(X_train, Y_train)\n",
    "    Y_pred = sgd.predict(X_test)\n",
    "    return Y_pred\n",
    "\n",
    "def naive_bayes_pred(X_train, Y_train, X_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    Naive Bayes using X_train and Y_train to predict labels of X_train\n",
    "    \"\"\"\n",
    "    nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "    nb.fit(X_train, Y_train)\n",
    "    Y_pred = nb.predict(X_test)\n",
    "    return Y_pred\n",
    "\n",
    "def classification_metrics(Y_pred, Y_true):\n",
    "    \n",
    "    accuracy = accuracy_score(Y_true, Y_pred)\n",
    "    precision = precision_score(Y_true, Y_pred)\n",
    "    recall = recall_score(Y_true, Y_pred)\n",
    "    f1 = f1_score(Y_true, Y_pred)\n",
    "    \n",
    "    #tn, fp, fn, tp = confusion_matrix(y_true, y_pred)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "    \n",
    "#input: Name of classifier, predicted labels, actual labels\n",
    "def display_metrics(classifierName, Y_pred, Y_true):\n",
    "    print(\"______________________________________________\")\n",
    "    print((\"Classifier: \"+classifierName))\n",
    "    acc, precision, recall, f1score = classification_metrics(Y_pred,Y_true)\n",
    "    print((\"Accuracy: \"+str(acc)))\n",
    "    print((\"Precision: \"+str(precision)))\n",
    "    print((\"Recall: \"+str(recall)))\n",
    "    print((\"F1-score: \"+str(f1score)))\n",
    "    print(\"______________________________________________\")\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "#x_train, x_test, y_train, y_test\n",
    "X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(notes, labels, test_size=0.33, random_state = seed)\n",
    "\n",
    "display_metrics(\"Logistic Regression\", logistic_regression_pred(X_train_b, y_train_b, X_test_b), y_test_b)\n",
    "display_metrics(\"SVM\",svm_pred(X_train_b, y_train_b, X_test_b),y_test_b)\n",
    "display_metrics(\"Naive Bayes\", naive_bayes_pred(X_train_b, y_train_b, X_test_b), y_test_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2b3371-d61e-4825-98da-ec8aa5eb4bfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6719bf80-f627-4200-9dbc-317c6a41b4ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
