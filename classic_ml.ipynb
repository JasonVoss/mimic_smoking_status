{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb48e8f-1a56-4eaf-afdb-066fbb4151bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: pyhealth in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (1.1.3)\n",
      "Requirement already satisfied: inflect in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (6.0.4)\n",
      "Requirement already satisfied: autocorrect in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (2.6.1)\n",
      "Requirement already satisfied: torchtext in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (0.14.1)\n",
      "Requirement already satisfied: gensim==3.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyhealth inflect autocorrect torchtext gensim==3.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4cdc8d-b444-4e13-b1ef-455b4ec33d02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyhealth.medcode import InnerMap\n",
    "from pyhealth.datasets import MIMIC4Dataset\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "import inflect\n",
    "from autocorrect import spell\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "import numpy as np\n",
    "import statistics\n",
    "# for progress bar\n",
    "from tqdm import tqdm_notebook\n",
    "import random\n",
    "import json\n",
    "import tqdm\n",
    "from sklearn.metrics import *\n",
    "\n",
    "# set seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18a8297-a8a3-439d-ba54-f27c403417ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54d7aed-2929-4636-83ef-a07e3fc2da0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('data/781/embeddings/tokenized_notes.pckl', 'rb') as f:\n",
    "    input_ids = pickle.load(f)\n",
    "\n",
    "with open('data/781/embeddings/embedding_matrix_GNV.pckl', 'rb') as f:\n",
    "    embedding_matrix_GNV = pickle.load(f)\n",
    "    embedding_matrix_GNV = torch.tensor(embedding_matrix_GNV)\n",
    "\n",
    "with open('data/781/embeddings/embedding_matrix_w2v.pckl', 'rb') as f:\n",
    "    embedding_matrix_w2v = pickle.load(f)\n",
    "    embedding_matrix_w2v = torch.tensor(embedding_matrix_w2v)\n",
    "\n",
    "with open('data/781/embeddings/word_index_eff.pckl', 'rb') as f:\n",
    "    word2idx = pickle.load(f)\n",
    "\n",
    "with open('data/781/embeddings/max_len_eff.pckl', 'rb') as f:\n",
    "    normal_max_len = pickle.load(f)\n",
    "\n",
    "with open('data/781/labels.pckl', 'rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "\n",
    "with open('data/781/df_notes_discharge.pckl', 'rb') as f:\n",
    "    df_notes_discharge = pickle.load(f)\n",
    "\n",
    "with open('data/781/cleaned_notes.pckl', 'rb') as f:\n",
    "    notes = pickle.load(f)\n",
    "    \n",
    "#with open('data/embeddings/pretrain.pckl', 'rb')\n",
    "#    pretrain = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccb3fa3-6af6-4902-b3ac-64fb2ce42636",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('pretrained GNV num embeddings ', len(embedding_matrix_GNV))\n",
    "print('GNV embedding dimensions ', len(embedding_matrix_GNV[0]))\n",
    "\n",
    "print('pretrained w2v num embeddings ', len(embedding_matrix_w2v))\n",
    "print('w2v embedding dimensions ', len(embedding_matrix_w2v[0]))\n",
    "\n",
    "print('len encoded notes, or total notes is ', len(input_ids))\n",
    "print('len or word index, or total unique words is ', len(word2idx))\n",
    "print('len of labels is', len(labels))\n",
    "print('len of first note is ', len(input_ids[0]))\n",
    "print('max len is ', normal_max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41360dab-19f5-406d-9ecd-099fa40e5ec5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import (TensorDataset, DataLoader, RandomSampler, SequentialSampler)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def data_loader(x_train, x_test, y_train, y_test, batch_size=8):\n",
    "    \"\"\"Convert train and test sets to tensors and load them to a dataLoader\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert data type to torch.Tensor\n",
    "    x_train, x_test, y_train, y_test = tuple(torch.tensor(data) for data in [x_train, x_test, y_train, y_test])\n",
    "\n",
    "    # Create DataLoader for training data\n",
    "    train_data = TensorDataset(x_train, y_train)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    # Create DataLoader for validation data\n",
    "    val_data = TensorDataset(x_test, y_test)\n",
    "    val_sampler = SequentialSampler(val_data)\n",
    "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "\n",
    " # Train Test Split\n",
    "x_train, x_test, y_train, y_test = train_test_split(input_ids, labels, test_size=0.33, random_state=seed)\n",
    "\n",
    "batch_size=8\n",
    "# Load data to PyTorch DataLoader\n",
    "train_dataloader, val_dataloader = data_loader(x_train, x_test, y_train, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36027d3b-33df-476e-973e-7765312fb822",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Classic machine learning\n",
    "\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer # add reference\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "def logistic_regression_pred(X_train, Y_train, X_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    logistic regression classifier using X_train and Y_train to predict labels of X_train\n",
    "    \"\"\"\n",
    "    logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5, penalty='l2', max_iter=1000)),\n",
    "               ])\n",
    "    logreg.fit(X_train, Y_train)\n",
    "    Y_pred = logreg.predict(X_test)\n",
    "    return Y_pred\n",
    "\n",
    "def svm_pred(X_train, Y_train, X_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    SVM classifier using X_train and Y_train to predict labels of X_train\n",
    "    \"\"\"\n",
    "\n",
    "    sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=seed, max_iter=5, tol=None)),\n",
    "                ])\n",
    "    sgd.fit(X_train, Y_train)\n",
    "    Y_pred = sgd.predict(X_test)\n",
    "    return Y_pred\n",
    "\n",
    "def naive_bayes_pred(X_train, Y_train, X_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    Naive Bayes using X_train and Y_train to predict labels of X_train\n",
    "    \"\"\"\n",
    "    nb = Pipeline([('vect', CountVectorizer()),\n",
    "                   #The multinomial distribution normally requires integer feature counts. fractional counts such as tf-idf might also work. \n",
    "                   #https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "                   #tdif is a normalization optimizaiton to reduce impact of very common words\n",
    "                   #in this case of NB on this vector tdif does not work as good, it results in all 0, as it is an optimization it is not necessary.\n",
    "               ('clf', MultinomialNB(alpha=1)),\n",
    "              ])\n",
    "    nb.fit(X_train, Y_train)\n",
    "    Y_pred = nb.predict(X_test)\n",
    "    return Y_pred\n",
    "\n",
    "def classification_metrics(Y_pred, Y_true):\n",
    "    \n",
    "    accuracy = accuracy_score(Y_true, Y_pred)\n",
    "    precision = precision_score(Y_true, Y_pred)\n",
    "    recall = recall_score(Y_true, Y_pred)\n",
    "    f1 = f1_score(Y_true, Y_pred)\n",
    "    \n",
    "    #tn, fp, fn, tp = confusion_matrix(y_true, y_pred)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "    \n",
    "#input: Name of classifier, predicted labels, actual labels\n",
    "def display_metrics(classifierName, Y_pred, Y_true):\n",
    "    print(\"______________________________________________\")\n",
    "    print((\"Classifier: \"+classifierName))\n",
    "    acc, precision, recall, f1score = classification_metrics(Y_pred,Y_true)\n",
    "    print((\"Accuracy: \"+str(acc)))\n",
    "    print((\"Precision: \"+str(precision)))\n",
    "    print((\"Recall: \"+str(recall)))\n",
    "    print((\"F1-score: \"+str(f1score)))\n",
    "    print(\"______________________________________________\")\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "#x_train, x_test, y_train, y_test\n",
    "X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(notes, labels, test_size=.33, random_state = seed)\n",
    "\n",
    "display_metrics(\"Logistic Regression\", logistic_regression_pred(X_train_b, y_train_b, X_test_b), y_test_b)\n",
    "display_metrics(\"SVM\",svm_pred(X_train_b, y_train_b, X_test_b),y_test_b)\n",
    "display_metrics(\"Naive Bayes\", naive_bayes_pred(X_train_b, y_train_b, X_test_b), y_test_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d75ca1-769a-4dcf-b794-b8edef2beac7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
