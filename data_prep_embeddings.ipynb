{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb48e8f-1a56-4eaf-afdb-066fbb4151bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pyhealth inflect autocorrect torchtext gensim==3.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4cdc8d-b444-4e13-b1ef-455b4ec33d02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyhealth.medcode import InnerMap\n",
    "from pyhealth.datasets import MIMIC4Dataset\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "import inflect\n",
    "from autocorrect import spell\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "import numpy as np\n",
    "import statistics\n",
    "# for progress bar\n",
    "from tqdm import tqdm_notebook\n",
    "import random\n",
    "import json\n",
    "import tqdm\n",
    "from sklearn.metrics import *\n",
    "\n",
    "# set seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddce720-0998-4719-85b0-6d9663e76e8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = MIMIC4Dataset(\n",
    "        root=\"data/mimic4_subset\",\n",
    "        tables=[\"diagnoses_icd\", \"procedures_icd\"],\n",
    "    )\n",
    "#dataset.stat()\n",
    "#dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e72fb8-896a-4bab-a9af-96fadd5b6aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "icd9cm = InnerMap.load(\"ICD9CM\")\n",
    "icd10cm = InnerMap.load(\"ICD10CM\")\n",
    "#smoker = icd9cm.lookup(\"V15.82\")\n",
    "\n",
    "counter = 0\n",
    "patient_dict = dataset.patients\n",
    "labels = []\n",
    "for subject_id, patient in patient_dict.items():\n",
    "    #if counter > 100:\n",
    "    #    break\n",
    "    #counter += 1\n",
    "    tobacco = 0\n",
    "    visit_dict = patient.visits\n",
    "    for visit_id, visit in visit_dict.items():\n",
    "        #print(visit.encounter_time, visit.available_tables)\n",
    "        '''\n",
    "        find the first occurence of identifying a smoker.  All other visits keep code as a smoker or former smoker, but to find this in the notes\n",
    "        after spot checking I see it is more often than not only written down as such the first time it is identified \n",
    "        '''\n",
    "        events = visit.get_event_list('diagnoses_icd')\n",
    "        for event in events:\n",
    "            if event.vocabulary == 'ICD9CM' and event.code in ['V1582', '3051']:\n",
    "                tobacco = 1\n",
    "                break\n",
    "                #explain = icd9cm.lookup(event.code)\n",
    "                #print(event.patient_id, event.visit_id, visit.encounter_time, event.vocabulary, event.code, explain)\n",
    "            elif event.vocabulary == 'ICD10CM' and event.code.startswith('F17'):\n",
    "                tobacco = 1\n",
    "                break\n",
    "                #explain = icd10cm.lookup(event.code)\n",
    "                #print(event.patient_id, event.visit_id, visit.encounter_time, event.vocabulary, event.code, explain)\n",
    "        if tobacco == 1:\n",
    "            break\n",
    "    labels.append({'subject_id':subject_id,'label':tobacco,'hadm_id':visit_id})\n",
    "\n",
    "label_df = pd.DataFrame(labels)\n",
    "label_df[\"subject_id\"] = pd.to_numeric(label_df[\"subject_id\"])\n",
    "label_df[\"hadm_id\"] = pd.to_numeric(label_df[\"hadm_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821bce96-924f-46f1-82c3-d73933ea8963",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function that cleans text\n",
    "# still need to account for contractions, abbreviations, and numbers/fractions\n",
    "default_stemmer = PorterStemmer()\n",
    "default_stopwords = stopwords.words('english') # or any other list of your choice\n",
    "def clean_text(text, replace_numbers = False, remove_rare = False, remove_punctuation = False, stem_text = False, remove_stopwords = False, remove_num = False , spell_check = False, remove_repeat = False):\n",
    "        def misc_cleaning(text):\n",
    "                text = re.sub(\"-([a-zA-Z]+)\", r\"\\1\", text) # replaces hyphen with spaces in case of strings\n",
    "                text = re.sub(' y ', '', text) # gets rid of random y accent stuff scattered through the text\n",
    "                text = re.sub('yyy', 'y', text)\n",
    "                text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "                text = re.sub(r\"what's\", \"what is \", text)\n",
    "                text = re.sub(r\"\\'s\", \" \", text)\n",
    "                text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "                text = re.sub(r\"can't\", \"cannot \", text)\n",
    "                text = re.sub(r\"n't\", \" not \", text)\n",
    "                text = re.sub(r\"i'm\", \"i am \", text)\n",
    "                text = re.sub(r\"\\'re\", \" are \", text)\n",
    "                text = re.sub(r\"\\'d\", \" would \", text)\n",
    "                text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "                text = re.sub(r\",\", \" \", text)\n",
    "                text = re.sub(r\"\\.\", \" \", text)\n",
    "                text = re.sub(r\"!\", \" ! \", text)\n",
    "                text = re.sub(r\"\\/\", \" \", text)\n",
    "                text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "                text = re.sub(r\"\\+\", \" + \", text)\n",
    "                text = re.sub(r\"\\-\", \" - \", text)\n",
    "                text = re.sub(r\"\\=\", \" = \", text)\n",
    "                text = re.sub(r\"'\", \" \", text)\n",
    "                text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "                text = re.sub(r\":\", \" : \", text)\n",
    "                text = re.sub(r\" e g \", \" eg \", text)\n",
    "                text = re.sub(r\" b g \", \" bg \", text)\n",
    "                text = re.sub(r\" u s \", \" american \", text)\n",
    "                text = re.sub(r\"\\0s\", \"0\", text)\n",
    "                text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "                text = re.sub(r\"e - mail\", \"email\", text)\n",
    "                text = re.sub(r\"j k\", \"jk\", text)\n",
    "                text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "                return text\n",
    "\n",
    "        # function to tokenize text which is used in a lot of the later processing\n",
    "        def tokenize_text(text):\n",
    "                return [w for s in sent_tokenize(text) for w in word_tokenize(s)]\n",
    "\n",
    "        text = text.strip(' ') # strip whitespaces\n",
    "        text = text.lower() # lowercase\n",
    "        text = misc_cleaning(text) # look at function, random cleaning stuff\n",
    "        \n",
    "        if remove_repeat:\n",
    "                sentences = sent_tokenize(text)\n",
    "                sentences = list(dict.fromkeys(sentences))\n",
    "                text = \" \".join(sentences)\n",
    "        \n",
    "        # removes punctuation\n",
    "        if remove_punctuation:\n",
    "                text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        # optional: replaces numbers (\"3\") with their word counterparts (\"three\")\n",
    "        if replace_numbers:\n",
    "                words = word_tokenize(text)\n",
    "                p = inflect.engine()\n",
    "                new_words = []\n",
    "                for word in words:\n",
    "                        if word.isdigit():\n",
    "                                new_word = p.number_to_words(word)\n",
    "                                new_words.append(new_word)\n",
    "                        else:\n",
    "                                new_words.append(word)\n",
    "                text = \" \".join(new_words)\n",
    "\n",
    "        # optional: removes the rarest words in each text --> right now it's 10\n",
    "        if remove_rare:\n",
    "                tokens = word_tokenize(text)\n",
    "                freq_dist = nltk.FreqDist(tokens)\n",
    "                rarewords = list(freq_dist.keys())[-10:]\n",
    "                new_words = [word for word in tokens if word not in rarewords]\n",
    "                text = \" \".join(new_words)\n",
    "\n",
    "        # optional: stems text using Porter Stemmer\n",
    "        if stem_text:\n",
    "                stemmer = default_stemmer\n",
    "                tokens = tokenize_text(text)\n",
    "                text = \" \".join([stemmer.stem(t) for t in tokens])\n",
    "\n",
    "        # removes stop words such as \"a\", \"the\", etc.\n",
    "        if remove_stopwords:\n",
    "                stop_words = default_stopwords\n",
    "                tokens = [w for w in tokenize_text(text) if w not in stop_words]\n",
    "                text = \" \".join(tokens)\n",
    "        \n",
    "        # optional: removes numbers completely from the ext\n",
    "        if remove_num:\n",
    "                text=text.split()\n",
    "                text=[x for x in text if not x.isnumeric()]\n",
    "                text= \" \".join(text)\n",
    "        \n",
    "        #remove headers from discharge notes\n",
    "        #name unit admission date discharge date date birth sex service medicine allergies known allergies adverse drug reactions attending chief complaint \n",
    "        headers = text.find(\" chief complaint \",1,300)\n",
    "        if headers > -1:\n",
    "            headers += 17\n",
    "            text = text[headers:]\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947d2ec1-217b-4cbd-a21f-f74adab16939",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_notes_discharge = pd.read_csv(\"data/mimic4_notes/discharge.csv\")\n",
    "#df_notes_discharge_detail = pd.read_csv(\"data/mimic4_notes/discharge_detail.csv\")\n",
    "#df_notes_radiology = pd.read_csv(\"data/mimic4_notes/radiology.csv\")\n",
    "\n",
    "print(df_notes_discharge.columns)\n",
    "print('total len', len(df_notes_discharge))\n",
    "#df_notes_discharge_detail.columns\n",
    "\n",
    "#print(df_notes_discharge['subject_id'])\n",
    "#print(df_notes_radiology['text'][0])\n",
    "\n",
    "#group by patient and concatenate all notes for one patient\n",
    "#df_notes_discharge = df_notes_discharge.groupby(['subject_id'], as_index = False).agg({'text': ' '.join})\n",
    "\n",
    "\n",
    "print(df_notes_discharge.columns)\n",
    "print('len of patients', len(df_notes_discharge))\n",
    "\n",
    "#trim down patients\n",
    "df_notes_discharge = df_notes_discharge.head(5000)\n",
    "print('final len', len(df_notes_discharge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fe066b-7c7d-4e66-a3e0-0f5407bd6505",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.multiprocessing import get\n",
    "\n",
    "#ddf = dd.from_pandas(df_notes_discharge, npartitions=7)\n",
    "#meta_df = pd.DataFrame(columns=[\"subject_id\", \"text\", \"new_text\"], dtype=object)\n",
    "\n",
    "#ddf['text'] = ddf['text'].apply(lambda text: clean_text(text, remove_punctuation = True, remove_stopwords = True, remove_repeat = True))\n",
    "\n",
    "#res = ddf.map_partitions(lambda df: df.assign(new_text = clean_text(df['text'], remove_punctuation = True, remove_stopwords = True, remove_repeat = True)), meta=meta_df)\n",
    "#res.to_csv(\"data/mimic4_notes/discharge_clean.csv\", index=False)\n",
    "\n",
    "#pandas_df = ddf.compute()\n",
    "#pandas_df.to_csv(\"data/mimic4_notes/discharge_clean.csv\", index=False)\n",
    "\n",
    "   \n",
    "# save labels of same size and order\n",
    "# return just the labels of the patients in the correct order as y, order of left table is maintained.\n",
    "#labels = df_notes_discharge.merge(label_df, on='subject_id', how='inner')['label']\n",
    "out_df = df_notes_discharge.merge(label_df, on=['subject_id','hadm_id'], how='inner')[['subject_id','hadm_id','label','text']]\n",
    "\n",
    "#limit to 781_unclean patients who have both discharge notes and codes for the same visit\n",
    "out_df = out_df.head(781)\n",
    "\n",
    "#HERE IS ABLATION OF CLEANING TEXT\n",
    "#out_df['text'] = out_df['text'].apply(lambda text: clean_text(text, remove_punctuation = True, remove_stopwords = True, remove_repeat = True, remove_num = True))\n",
    "\n",
    "labels_out = out_df['label'].to_numpy()\n",
    "#save notes for embeddings\n",
    "notes = list(out_df['text'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f5a032-33f9-4001-8df5-cf0578dc3f2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TESTING\n",
    "#tables=[\"diagnoses_icd\", \"procedures_icd\"],\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(len(labels_out))\n",
    "print(sum(labels_out))\n",
    "print('positivity rate', sum(labels_out) / len(labels_out))\n",
    "\n",
    "print(labels[0])\n",
    "print(icd9cm.lookup('V1582'))\n",
    "\n",
    "find_pat = pd.read_csv(\"data/mimic4_subset/diagnoses_icd.csv\")\n",
    "print(find_pat.columns)\n",
    "out = find_pat.loc[(find_pat['subject_id'] == 10000032) & (find_pat['hadm_id'] == 22595853)]\n",
    "print(out)\n",
    "\n",
    "print(df_notes_discharge.columns)\n",
    "print(label_df.columns)\n",
    "\n",
    "out = out_df.loc[(out_df['subject_id'] == 10000032) & (out_df['hadm_id'] == 22595853)]\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39e2a04-0959-4cce-b616-dce23b1a7543",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# process notes converted to index array of numbers of same length\n",
    "\n",
    "# transforms text to a sequence of integers padded to same length\n",
    "#from keras.preprocessing.text import Tokenizer\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def textTokenize(notes):\n",
    "    \"\"\"For each patients text, find max length, build a dict of words\n",
    "    \n",
    "    Returns:\n",
    "        tokenized_texts (List[List[str]]): List of list of tokens\n",
    "        word2idx (Dict): Dict built from the corpus\n",
    "        max_len (int): max sentence length\n",
    "    \"\"\"\n",
    "    t = get_tokenizer(\"basic_english\")\n",
    "    lengths = []\n",
    "    tokenized_texts = []\n",
    "    word2idx = {}\n",
    "    # Add padding and unknown tokens to the dictionary\n",
    "    word2idx['<pad>'] = 0\n",
    "    word2idx['<unk>'] = 1\n",
    "    idx = 2\n",
    "    for text in notes:\n",
    "        tokenized_text = t(text)\n",
    "        tokenized_texts.append(tokenized_text)\n",
    "        # Add new token to `word2idx`\n",
    "        for token in tokenized_text:\n",
    "            if token not in word2idx:\n",
    "                word2idx[token] = idx\n",
    "                idx += 1\n",
    "        lengths.append(len(tokenized_text))\n",
    "    mean_len = np.mean(lengths)\n",
    "    std_len = np.std(lengths)\n",
    "    max_len = np.max(lengths)\n",
    "    return tokenized_texts, word2idx, max_len, mean_len, std_len\n",
    "\n",
    "def encodeTokenizedText(tokenized_texts, word2idx, max_len):\n",
    "    \"\"\"Pad each sentence to the max length and encode tokens to their index in the all words dict.\n",
    "    Make it more efficient -  instead of max length, make it mean len + 4x std dev, to eliminate few outliers\n",
    "\n",
    "    Returns:\n",
    "        input_ids (np.array): Array of token indexes in the vocabulary with\n",
    "            shape (N, max_len). It will the input to the CNN.\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    rev_input_ids = []\n",
    "    masks = []\n",
    "    for tokenized_sent in tokenized_texts:\n",
    "        # Pad sentences to max_len\n",
    "        if (len(tokenized_sent) > max_len):\n",
    "            tokenized_sent = tokenized_sent[0:max_len]\n",
    "            mask = np.ones(len(tokenized_sent))\n",
    "        else:\n",
    "            \n",
    "            #use pre-padding\n",
    "            #tokenized_sent = ['<pad>'] * (max_len - len(tokenized_sent)) + tokenized_sent\n",
    "            #use post-padding ( will mask for rnn and bi directional rnn !)\n",
    "            pad = max_len - len(tokenized_sent)\n",
    "            mask = np.concatenate([np.ones(len(tokenized_sent)), np.zeros(pad)])\n",
    "            rev_tokenized_sent = tokenized_sent.copy()\n",
    "            rev_tokenized_sent.reverse()\n",
    "            tokenized_sent += ['<pad>'] * pad\n",
    "            rev_tokenized_sent += ['<pad>'] * pad\n",
    "            \n",
    "        if len(tokenized_sent) != max_len or len(mask) != max_len or len(rev_tokenized_sent) != max_len:\n",
    "            print(max_len, len(tokenized_sent), len(mask))\n",
    "            break\n",
    "            \n",
    "        # Encode tokens to input_ids, input_id is just the idx position when it was inserted, so it converts words to numbers\n",
    "        input_id = [word2idx.get(token) for token in tokenized_sent]\n",
    "        rev_input_id = [word2idx.get(token) for token in rev_tokenized_sent]\n",
    "        input_ids.append(input_id)\n",
    "        rev_input_ids.append(rev_input_id)\n",
    "        # Masks of same size to track masking\n",
    "        masks.append(mask)\n",
    "        \n",
    "    return np.array(input_ids, dtype=int), np.array(rev_input_ids, dtype=int), np.array(masks, dtype=int)\n",
    "\n",
    "\n",
    "tokenized_texts, word2idx, max_len, mean_len, std_len = textTokenize(notes)\n",
    "normal_max_len = int((mean_len + 4*std_len) + 1)\n",
    "max_len = normal_max_len\n",
    "\n",
    "# input_ids are the input to cnn and rnn models, as the tokenized text\n",
    "input_ids, rev_input_ids, masks = encodeTokenizedText(tokenized_texts, word2idx, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7514f3-5cd8-4bba-bcc0-2282c1e9d817",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(input_ids), len(rev_input_ids), len(masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4741adc6-abdb-4067-8204-1579a03bdcc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MAKE EMBEDDING MATRIX\n",
    "import gensim.downloader as api\n",
    "\n",
    "#pretrain = api.load('word2vec-google-news-300')\n",
    "\n",
    "with open('data/embeddings/pretrain.pckl', 'rb') as f:\n",
    "    pretrain = pickle.load(f)\n",
    "\n",
    "# Make Word2Vec embeddings from the notes themselves\n",
    "def make_w2v_model(notes, window, workers, epochs, vector_size, min_count):\n",
    "    model = gensim.models.Word2Vec(notes, size=vector_size, window=window, min_count=min_count, workers=workers)\n",
    "    print('Start training process...') \n",
    "    model.train(notes,total_examples=len(notes),epochs=epochs)\n",
    "    model.save(\"w2v.model\")\n",
    "    print(\"Model Saved\")\n",
    "\n",
    "make_w2v_model(notes,  window=5, workers=1, epochs=20, vector_size=300, min_count=2)\n",
    "\n",
    "def word_Embed_w2v(word_index, model):   \n",
    "    w2v = model\n",
    "    #convert pretrained word embedding to a dictionary\n",
    "    embedding_index=dict()\n",
    "    print('word vectors len is ',len(w2v.wv.vocab))\n",
    "    for i in range(len(w2v.wv.vocab)):\n",
    "        word=w2v.wv.index2word[i]\n",
    "        if word is not None:\n",
    "            embedding_index[word]=w2v.wv[word]  \n",
    "    #extract word embedding for train and test data\n",
    "    \n",
    "    # create matrix of shape\n",
    "    embedding_matrix = np.random.uniform(-0.25, 0.25, (len(word_index), 300))    \n",
    "    embedding_matrix[word_index['<pad>']] = np.zeros((300,))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def word_Embed_GNV(word_index):   \n",
    "    \"\"\" Load the pretrained vectors for each token in our vocabulary. \n",
    "    For tokens with no pretraiend vectors, we will initialize random word vectors with the same length and variance.\n",
    "    \n",
    "     Returns:\n",
    "        embeddings (np.array): Embedding matrix with shape (N, d) where N is\n",
    "            the size of word2idx and d is embedding dimension\n",
    "    \"\"\"\n",
    "    #pretrain = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "    # convert pretrained word embedding to a dictionary\n",
    "    print('pretrain len is ',len(pretrain.wv.vocab))\n",
    "    # fill embedding_index with every word from the pretrain\n",
    "    embedding_index=dict()\n",
    "    for i in range(len(pretrain.wv.vocab)):\n",
    "        word=pretrain.wv.index2word[i]\n",
    "        if word is not None:\n",
    "            embedding_index[word]=pretrain.wv[word] \n",
    "            \n",
    "    # create matrix of shape\n",
    "    embedding_matrix = np.random.uniform(-0.25, 0.25, (len(word_index), 300))    \n",
    "    embedding_matrix[word_index['<pad>']] = np.zeros((300,))\n",
    "    \n",
    "    for word, i in tqdm_notebook(word_index.items()):\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "w2v_model = Word2Vec.load(\"w2v.model\")\n",
    "embedding_matrix_w2v = word_Embed_w2v(word2idx, w2v_model)\n",
    "\n",
    "embedding_matrix_GNV = word_Embed_GNV(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051fe08f-2838-41d3-b3f6-c96386745d4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#dump encoded notes and embeddings\n",
    "with open(\"data/781_unclean/labels.pckl\", \"wb\") as f:\n",
    "    pickle.dump(labels_out, f)\n",
    "print(\"Saved labels\")\n",
    "\n",
    "# save cleaned notes into a pickle file\n",
    "with open('data/781_unclean/cleaned_notes.pckl', 'wb') as f:\n",
    "    pickle.dump(notes, f)\n",
    "print(\"Saved cleansed notes\")\n",
    "\n",
    "with open('data/781_unclean/df_notes_discharge.pckl', 'wb') as f:\n",
    "    pickle.dump(df_notes_discharge, f)\n",
    "print(\"Saved cleansed df_notes_discharge \")\n",
    "\n",
    "with open('data/781_unclean/embeddings/pretrain.pckl', 'wb') as f:\n",
    "    pickle.dump(pretrain, f)\n",
    "print(\"Saved pretrain\")\n",
    "\n",
    "with open('data/781_unclean/embeddings/tokenized_notes.pckl', 'wb') as f:\n",
    "    pickle.dump(input_ids, f)\n",
    "print(\"Saved Tokenized Notes\")\n",
    "\n",
    "with open('data/781_unclean/embeddings/rev_tokenized_notes.pckl', 'wb') as f:\n",
    "    pickle.dump(rev_input_ids, f)\n",
    "print(\"Saved Reverse Tokenized Notes\")\n",
    "\n",
    "with open('data/781_unclean/embeddings/masks.pckl', 'wb') as f:\n",
    "    pickle.dump(masks, f)\n",
    "print(\"Saved Masks\")\n",
    "\n",
    "with open('data/781_unclean/embeddings/embedding_matrix_GNV.pckl', 'wb') as f:\n",
    "    pickle.dump(embedding_matrix_GNV, f)\n",
    "print(\"Saved Google Vector Word Embedding Matrix\")\n",
    "\n",
    "with open('data/781_unclean/embeddings/embedding_matrix_w2v.pckl', 'wb') as f:\n",
    "    pickle.dump(embedding_matrix_w2v, f)\n",
    "print(\"Saved Word 2 Vector Embedding Matrix\")\n",
    "\n",
    "with open('data/781_unclean/embeddings/word_index_eff.pckl', 'wb') as f:\n",
    "    pickle.dump(word2idx, f)\n",
    "print(\"Saved Word Indices\")\n",
    "\n",
    "with open('data/781_unclean/embeddings/max_len_eff.pckl', 'wb') as f:\n",
    "    pickle.dump(max_len, f)\n",
    "print(\"Saved Maximum Length of One Patient's Notes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2e1b9f-f664-4d85-ba4b-2c30328a9225",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sum(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c79289-e236-485b-87e4-70b443db0e71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sum(rev_input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5611f5e9-1246-4944-838f-146142312fd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('data/781_unclean/embeddings/test_input.json', 'w') as f:\n",
    "    json.dump(input_ids.tolist(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfd9a5b-95ae-479d-83fa-fff0b3f4373b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e403ae7-a3b1-418e-b09e-2488241a1790",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
