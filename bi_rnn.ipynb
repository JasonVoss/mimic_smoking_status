{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb48e8f-1a56-4eaf-afdb-066fbb4151bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pyhealth inflect autocorrect torchtext gensim==3.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4cdc8d-b444-4e13-b1ef-455b4ec33d02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyhealth.medcode import InnerMap\n",
    "from pyhealth.datasets import MIMIC4Dataset\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "import inflect\n",
    "from autocorrect import spell\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "import numpy as np\n",
    "import statistics\n",
    "# for progress bar\n",
    "from tqdm import tqdm_notebook\n",
    "import random\n",
    "import json\n",
    "import tqdm\n",
    "from sklearn.metrics import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# set seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18a8297-a8a3-439d-ba54-f27c403417ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54d7aed-2929-4636-83ef-a07e3fc2da0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('data/embeddings/rev_tokenized_notes.pckl', 'rb') as f:\n",
    "    rev_input_ids = pickle.load(f)\n",
    "    \n",
    "with open('data/embeddings/tokenized_notes.pckl', 'rb') as f:\n",
    "    input_ids = pickle.load(f)\n",
    "    \n",
    "with open('data/embeddings/masks.pckl', 'rb') as f:\n",
    "    masks = pickle.load(f)\n",
    "\n",
    "with open('data/embeddings/embedding_matrix_GNV.pckl', 'rb') as f:\n",
    "    embedding_matrix_GNV = pickle.load(f)\n",
    "    embedding_matrix_GNV = torch.tensor(embedding_matrix_GNV)\n",
    "\n",
    "with open('data/embeddings/embedding_matrix_w2v.pckl', 'rb') as f:\n",
    "    embedding_matrix_w2v = pickle.load(f)\n",
    "    embedding_matrix_w2v = torch.tensor(embedding_matrix_w2v)\n",
    "\n",
    "with open('data/embeddings/word_index_eff.pckl', 'rb') as f:\n",
    "    word2idx = pickle.load(f)\n",
    "\n",
    "with open('data/embeddings/max_len_eff.pckl', 'rb') as f:\n",
    "    normal_max_len = pickle.load(f)\n",
    "\n",
    "with open('data/labels.pckl', 'rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "\n",
    "with open('data/df_notes_discharge.pckl', 'rb') as f:\n",
    "    df_notes_discharge = pickle.load(f)\n",
    "    \n",
    "#with open('data/embeddings/pretrain.pckl', 'rb')\n",
    "#    pretrain = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccb3fa3-6af6-4902-b3ac-64fb2ce42636",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('pretrained GNV num embeddings ', len(embedding_matrix_GNV))\n",
    "print('GNV embedding dimensions ', len(embedding_matrix_GNV[0]))\n",
    "\n",
    "print('pretrained w2v num embeddings ', len(embedding_matrix_w2v))\n",
    "print('w2v embedding dimensions ', len(embedding_matrix_w2v[0]))\n",
    "\n",
    "print('len encoded notes, or total notes is ', len(input_ids))\n",
    "print('len of first note is ', len(input_ids[0]))\n",
    "print('max len is ', normal_max_len)\n",
    "print('len or word index, or total bique words is ', len(word2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105e899d-68d2-403a-add7-42b8a105e709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41360dab-19f5-406d-9ecd-099fa40e5ec5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import (TensorDataset, DataLoader, RandomSampler, SequentialSampler)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def data_loader(x_train, x_test, rev_x_train, rev_x_test, masks_train, masks_test, y_train, y_test, batch_size=8):\n",
    "    \"\"\"Convert train and test sets to tensors and load them to a dataLoader\n",
    "    \"\"\"\n",
    "\n",
    "    # reverse the sequence of word tokens for a patient.  x is from input_ids. x shape is (N, max_len)\n",
    "    #rev_x_train = np.fliplr(x_train)\n",
    "    #rev_x_test = np.fliplr(x_test)\n",
    "    #rev_masks_train = np.fliplr(x_train)\n",
    "    #rev_masks_test = np.fliplr(x_test)\n",
    "    # copy to get rid of negative strides\n",
    "    #rev_x_train = rev_x_train.copy()\n",
    "    #rev_x_test = rev_x_test.copy()\n",
    "    #rev_masks_train = rev_masks_train.copy()\n",
    "    #rev_masks_test = rev_masks_test.copy()\n",
    "    \n",
    "    # we have padding at end of rev_x input arg\n",
    "    \n",
    "    # Convert data type to torch.Tensor\n",
    "    x_train, rev_x_train, x_test, rev_x_test, masks_train, masks_test, y_train, y_test = tuple(torch.tensor(data) for data in [x_train, rev_x_train, x_test, rev_x_test, masks_train, masks_test, y_train, y_test])\n",
    "    #x_train, x_test, y_train, y_test = tuple(torch.tensor(data) for data in [x_train, x_test, y_train, y_test])\n",
    "\n",
    "    # Create DataLoader for training data\n",
    "    train_data = TensorDataset(x_train, rev_x_train, masks_train, y_train)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    # Create DataLoader for validation data\n",
    "    val_data = TensorDataset(x_test, rev_x_test, masks_test, y_test)\n",
    "    val_sampler = SequentialSampler(val_data)\n",
    "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "    \n",
    " # Train Test Split\n",
    "x_train, x_test, rev_x_train, rev_x_test, masks_train, masks_test, y_train, y_test = train_test_split(input_ids, rev_input_ids, masks, labels, test_size=0.33, random_state=seed)\n",
    "\n",
    "batch_size=8\n",
    "# Load data to PyTorch DataLoader\n",
    "train_dataloader, val_dataloader = data_loader(x_train, x_test, rev_x_train, rev_x_test, masks_train, masks_test, y_train, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fdccc2-e25d-491c-976e-6d3b9b9a26f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_last_word_before_pad(output, masks):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        hidden_states: the hidden states of each encoded note sequence of shape (batch_size, notes sequence size, hidden_size)\n",
    "        masks: the padding masks of shape (batch_size, notes sequence size)\n",
    "\n",
    "    Outputs:\n",
    "        last_hidden_state: the hidden state for the last word before padding of shape (batch_size, hidden_size)\n",
    "        \n",
    "    First convert the mask to a vector of shape (batch_size,) containing the true notes length; \n",
    "          and then use this length vector as index to select the last word.\n",
    "    \"\"\"\n",
    "    \n",
    "    #masks_test = torch.argmin(masks, dim=1, keepdim=True)[0]\n",
    "    #print('first zero mask',masks[0][masks_test])\n",
    "    #print('last one mask',masks[0][masks_test - 1])\n",
    "    \n",
    "    lv_idx = torch.argmin(masks, dim=1, keepdim=True)\n",
    "    lv_idx = lv_idx.reshape(lv_idx.shape[0])\n",
    "    #print('lv_idx',lv_idx)\n",
    "    return output[range(output.shape[0]),lv_idx,:]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7da394a-1df1-4649-ac9f-ac02822acd35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BidirectionalRNN(nn.Module):\n",
    "\n",
    "    def __init__(self\n",
    "                 ,pretrained_embedding\n",
    "                 ,num_classes=2\n",
    "                 ,dropout=0.2\n",
    "                 ,freeze_embedding=False\n",
    "                 ,embedding_dim=300\n",
    "                 ,conv_reduce=True\n",
    "                 ,conv_pool=2\n",
    "                 ,num_words=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.name = \"bi_rnn_model\"\n",
    "\n",
    "        # len of word index is same as len of embedding matrix\n",
    "        # embeddings\n",
    "        # we have 2 different pretrained, always using pretrained for this analysis, num words found in pretrained embedding matrix shape (equal to len of word idx)\n",
    "        if pretrained_embedding is not None:\n",
    "            self.vocab_size, self.embed_dim = pretrained_embedding.shape\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_embedding, freeze=freeze_embedding)\n",
    "        else:\n",
    "            self.embed_dim = embed_dim\n",
    "            self.embedding = nn.Embedding(num_embeddings=num_words, embedding_dim=self.embed_dim, padding_idx=0)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.conv_reduce = conv_reduce\n",
    "        self.input_size = self.embed_dim\n",
    "        self.hidden_size = self.embed_dim\n",
    "        if (conv_reduce):\n",
    "            self.conv1 =  nn.Conv1d(self.input_size, 32, kernel_size=3)\n",
    "            self.pool = nn.MaxPool1d(conv_pool)\n",
    "            self.input_size = 32\n",
    "            self.hidden_size = 128\n",
    "        \n",
    "        # bidirectional is false by default, but I want to be clear I am handling this to apply masks\n",
    "        self.rnn = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_size, batch_first=True, bidirectional=False)\n",
    "        #batch_first â€“ If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature)\n",
    "        \n",
    "        # we are always using binomial, ICD9 is much more common in dataset than ICD10\n",
    "        if num_classes == 2:\n",
    "            #binary\n",
    "            self.fc = nn.Linear(self.hidden_size * 2, 1)\n",
    "            self.out = nn.Sigmoid()\n",
    "        else:\n",
    "            self.fc = nn.Linear(self.hidden_size * 2, num_classes)\n",
    "            self.out = nn.Softmax()         \n",
    "            \n",
    "        nn.init.kaiming_normal_(self.fc.weight, nonlinearity='relu')         \n",
    "            \n",
    "    \n",
    "    def forward(self, x, rev_x, masks):\n",
    "        '''\n",
    "        Arguments:\n",
    "            x: the tokenized notes or input_ids of shape (total notes, max_len)\n",
    "\n",
    "        Outputs:\n",
    "            probs: probabilities of shape (batch_size)\n",
    "        '''\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        x = self.embedding(x).float()\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        if self.conv_reduce:\n",
    "            # Permute to match input shape requirement of nn.Conv1d. Output shape: (b, embed_dim, max_len)\n",
    "            x = x.permute(0, 2, 1)\n",
    "            x = F.relu(self.conv1(x))\n",
    "            x = self.pool(x)\n",
    "            masks = self.pool(masks.float()).int()\n",
    "            # permute back to original\n",
    "            x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # example input shape to LSTM\n",
    "        # without conv reduce input should be (batch_size, max_len, embedding_dim) (8, 16410, 300)\n",
    "        # if conv_reduce and pool then it becomes (8, 8204, 32)\n",
    "        \n",
    "        output, (hn, cn) = self.rnn(x) \n",
    "        true_h_n = find_last_word_before_pad(output, masks)\n",
    "        if not self.conv_reduce:\n",
    "            #activation not applied after conv so apply here\n",
    "            true_h_n = F.relu(true_h_n)\n",
    "        '''\n",
    "        repeat for the reverse order (rev_x)\n",
    "        '''\n",
    "        rev_x = self.embedding(rev_x).float()\n",
    "        rev_x = self.dropout(rev_x)\n",
    "        \n",
    "        if self.conv_reduce:\n",
    "            # Permute to match input shape requirement of nn.Conv1d. Output shape: (b, embed_dim, max_len)\n",
    "            rev_x = rev_x.permute(0, 2, 1)\n",
    "            rev_x = F.relu(self.conv1(rev_x))\n",
    "            rev_x = self.pool(rev_x)\n",
    "            masks = self.pool(masks.float()).int()\n",
    "            # permute back to original\n",
    "            rev_x = rev_x.permute(0, 2, 1)\n",
    "        \n",
    "        # example input shape to LSTM\n",
    "        # without conv reduce input should be (batch_size, max_len, embedding_dim) (8, 16410, 300)\n",
    "        # if conv_reduce and pool then it becomes (8, 8204, 32)\n",
    "        \n",
    "        rev_output, (rev_hn, rev_cn) = self.rnn(rev_x)\n",
    "        rev_true_h_n = find_last_word_before_pad(rev_output, masks)\n",
    "        if not self.conv_reduce:\n",
    "            #activation not applied after conv so apply here\n",
    "            rev_true_h_n = F.relu(rev_true_h_n)\n",
    "        '''\n",
    "            concatenate the hidden states for both directions\n",
    "        '''  \n",
    "\n",
    "        both = torch.cat([true_h_n, rev_true_h_n], 1)\n",
    "        x = self.fc(both)\n",
    "        x = self.out(x).view(batch_size)\n",
    "        return x     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfdea02-e530-433d-bb7c-56f4f6fb41a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, loss, filename='models/bi_rnn_model_checkpoint.torch'):\n",
    "    torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                }, filename)\n",
    "\n",
    "def load_checkpoint(model, optimizer, filename='models/bi_rnn_model_checkpoint.torch'):\n",
    "    # Note: Input model & optimizer should be pre-defined.  This routine only updates their states.\n",
    "    epoch = 0\n",
    "    if os.path.isfile(filename):\n",
    "        print(\"=> loading checkpoint '{}'\".format(filename))\n",
    "        checkpoint = torch.load(filename)\n",
    "        epoch = checkpoint['epoch']\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {}) loss {}\"\n",
    "                  .format(filename, checkpoint['epoch'], checkpoint['loss']))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(filename))\n",
    "\n",
    "    return model, optimizer, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b254a50b-83e6-400e-bf62-b704a93a3e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=2, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "        self.prev_validation_loss = np.inf\n",
    "\n",
    "    def early_stop_min(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.prev_validation_loss:\n",
    "            self.counter = 0\n",
    "        if validation_loss > (self.prev_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "            \n",
    "        self.prev_validation_loss = validation_loss\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07662a2-b54c-490b-be4b-8d84dbdb4eba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_model(model, dataloader):\n",
    "    model.eval()\n",
    "    Y_pred = []\n",
    "    Y_true = []\n",
    "    batch_loss = []\n",
    "    with torch.no_grad():\n",
    "        for x, rev_x, masks, target in dataloader:\n",
    "            # your code here\n",
    "            Y_true.append(target)\n",
    "            Y_scores = model(x, rev_x, masks)\n",
    "            loss = criterion(Y_scores, target.float())\n",
    "            batch_loss.append(loss.cpu().data.numpy())\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            predicted = (Y_scores > .5).int()\n",
    "            Y_pred.append(predicted)\n",
    "        val_loss = np.mean(batch_loss)\n",
    "        Y_pred = np.concatenate(Y_pred, axis=0)\n",
    "        Y_true = np.concatenate(Y_true, axis=0)\n",
    "        #f1 precision recall accuracy\n",
    "        p, r, f, _ = precision_recall_fscore_support(Y_true, Y_pred, average='weighted')\n",
    "        a = accuracy_score(Y_true, Y_pred)\n",
    "    \n",
    "    return val_loss, p, r, f, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e28741-0c28-4f77-9760-109ecebafcce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TRAIN\n",
    "\n",
    "n_epochs = 50\n",
    "\n",
    "def train_model(model, train_dataloader, current_epoch=0, n_epochs=n_epochs, optimizer=None, criterion=None):\n",
    "\n",
    "    model.train() # prep model for training\n",
    "    last_epoch = 0\n",
    "    last_epoch_loss = 0\n",
    "    early_stopper = EarlyStopper(patience=2, min_delta=0)\n",
    "    for epoch in range(current_epoch, current_epoch + n_epochs):\n",
    "        last_epoch = epoch + 1\n",
    "        curr_epoch_loss = []\n",
    "        #count = 0\n",
    "        #for data, rev_data, target in train_dataloader:\n",
    "        for x_train, rev_x_train, masks_train, target in train_dataloader:\n",
    "            #if count > 1:\n",
    "            #    break\n",
    "            #count = 1\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            #outputs = model(data, rev_data)\n",
    "            outputs = model(x_train, rev_x_train, masks_train)\n",
    "            loss = criterion(outputs, target.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            curr_epoch_loss.append(loss.cpu().data.numpy())\n",
    "            \n",
    "        last_epoch_loss = np.mean(curr_epoch_loss)\n",
    "        echo_out = f\"Epoch {last_epoch}: curr_epoch_loss={last_epoch_loss}\"\n",
    "        print(echo_out)\n",
    "        with open(model.name + \"train_out\", 'a') as f:\n",
    "            f.write(echo_out + '\\n')\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            val_loss, p, r, f1, a = eval_model(model, val_dataloader)\n",
    "            val_echo_out = 'Epoch: {} \\t Validation loss: {:.2f}, precision: {:.2f}, recall:{:.2f}, f1_score: {:.2f}, accuracy: {:.2f}'.format(epoch + 1, val_loss, p, r, f1, a)\n",
    "            print(val_echo_out)\n",
    "            with open(model.name + \"train_out\", 'a') as f:\n",
    "                f.write(val_echo_out + '\\n')\n",
    "                \n",
    "        if early_stopper.early_stop(last_epoch_loss):\n",
    "            val_loss, p, r, f1, a = eval_model(model, val_dataloader)\n",
    "            val_echo_out = 'Epoch: {} \\t Validation loss: {:.2f}, precision: {:.2f}, recall:{:.2f}, f1_score: {:.2f}, accuracy: {:.2f}'.format(epoch + 1, val_loss, p, r, f1, a)\n",
    "            print(val_echo_out)\n",
    "            with open(model.name + \"train_out\", 'a') as f:\n",
    "                f.write(val_echo_out + '\\n')\n",
    "                f.write('EARLY STOP\\n')\n",
    "            break\n",
    "            \n",
    "    return model, last_epoch, last_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901bf17c-042a-45dd-b3d1-a60fd79f7c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "continued = False\n",
    "\n",
    "if continued:\n",
    "    with open('models/bi_rnn_model.pckl', 'rb') as f:\n",
    "        bi_rnn_model = pickle.load(f)\n",
    "    bi_rnn_optimizer = torch.optim.Adam(bi_rnn_model.parameters(), lr=0.001, betas=(0.9, 0.999), weight_decay=0.0, amsgrad=False)\n",
    "    cnn_model, cnn_optimizer, current_epoch = load_checkpoint(bi_rnn_model, bi_rnn_optimizer)\n",
    "else:\n",
    "    bi_rnn_model = BidirectionalRNN(pretrained_embedding = embedding_matrix_GNV, conv_reduce=False)\n",
    "    bi_rnn_optimizer = torch.optim.Adam(bi_rnn_model.parameters(), lr=0.001, betas=(0.9, 0.999), weight_decay=0.0, amsgrad=False)\n",
    "    current_epoch = 0\n",
    "    \n",
    "bi_rnn_model, last_epoch, last_epoch_loss = train_model(bi_rnn_model, train_dataloader, current_epoch=current_epoch, optimizer=bi_rnn_optimizer, criterion=criterion)\n",
    "\n",
    "with open('models/bi_rnn_model.pckl', 'wb') as f:\n",
    "    pickle.dump(bi_rnn_model, f)\n",
    "\n",
    "save_checkpoint(bi_rnn_model, bi_rnn_optimizer, last_epoch, last_epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962fb5a5-65db-42fb-ad35-7450fd5f9c69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
