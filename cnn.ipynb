{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cb48e8f-1a56-4eaf-afdb-066fbb4151bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: pyhealth in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (1.1.3)\n",
      "Requirement already satisfied: inflect in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (6.0.4)\n",
      "Requirement already satisfied: autocorrect in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (2.6.1)\n",
      "Requirement already satisfied: torchtext in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (0.14.1)\n",
      "Requirement already satisfied: gensim==3.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (3.6.0)\n",
      "Requirement already satisfied: six>=1.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from gensim==3.6.0) (1.16.0)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from gensim==3.6.0) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from gensim==3.6.0) (1.23.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from gensim==3.6.0) (1.8.1)\n",
      "Requirement already satisfied: scikit-learn>=0.24.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pyhealth) (1.0)\n",
      "Requirement already satisfied: torch>=1.8.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pyhealth) (1.13.1)\n",
      "Requirement already satisfied: pandas>=1.3.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pyhealth) (1.4.4)\n",
      "Requirement already satisfied: rdkit>=2022.03.4 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pyhealth) (2023.3.1)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pyhealth) (4.63.2)\n",
      "Requirement already satisfied: networkx>=2.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pyhealth) (3.0)\n",
      "Requirement already satisfied: pydantic>=1.9.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from inflect) (1.10.4)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from torchtext) (2.28.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pandas>=1.3.2->pyhealth) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pandas>=1.3.2->pyhealth) (2022.7)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pydantic>=1.9.1->inflect) (4.4.0)\n",
      "Requirement already satisfied: Pillow in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from rdkit>=2022.03.4->pyhealth) (9.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from scikit-learn>=0.24.2->pyhealth) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from scikit-learn>=0.24.2->pyhealth) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->torchtext) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->torchtext) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->torchtext) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->torchtext) (1.26.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyhealth inflect autocorrect torchtext gensim==3.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf4cdc8d-b444-4e13-b1ef-455b4ec33d02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyhealth.medcode import InnerMap\n",
    "from pyhealth.datasets import MIMIC4Dataset\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "import inflect\n",
    "from autocorrect import spell\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "import numpy as np\n",
    "import statistics\n",
    "# for progress bar\n",
    "from tqdm import tqdm_notebook\n",
    "import random\n",
    "import json\n",
    "import tqdm\n",
    "from sklearn.metrics import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# set seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18a8297-a8a3-439d-ba54-f27c403417ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b54d7aed-2929-4636-83ef-a07e3fc2da0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data prep\n",
    "with open('data/8000/labels.pckl', 'rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "\n",
    "with open('data/8000/df_notes_discharge.pckl', 'rb') as f:\n",
    "    df_notes_discharge = pickle.load(f)\n",
    "    \n",
    "with open('data/8000/cleaned_notes.pckl', 'rb') as f:\n",
    "    notes = pickle.load(f)\n",
    "\n",
    "# embeddings \n",
    "with open('data/8000/embeddings/tokenized_notes.pckl', 'rb') as f:\n",
    "    input_ids = pickle.load(f)\n",
    "\n",
    "with open('data/8000/embeddings/embedding_matrix_GNV.pckl', 'rb') as f:\n",
    "    embedding_matrix_GNV = pickle.load(f)\n",
    "    embedding_matrix_GNV = torch.tensor(embedding_matrix_GNV)\n",
    "\n",
    "with open('data/8000/embeddings/embedding_matrix_w2v.pckl', 'rb') as f:\n",
    "    embedding_matrix_w2v = pickle.load(f)\n",
    "    embedding_matrix_w2v = torch.tensor(embedding_matrix_w2v)\n",
    "\n",
    "with open('data/8000/embeddings/word_index_eff.pckl', 'rb') as f:\n",
    "    word2idx = pickle.load(f)\n",
    "\n",
    "with open('data/8000/embeddings/max_len_eff.pckl', 'rb') as f:\n",
    "    normal_max_len = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ccb3fa3-6af6-4902-b3ac-64fb2ce42636",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained GNV num embeddings  61122\n",
      "GNV embedding dimensions  300\n",
      "pretrained w2v num embeddings  61122\n",
      "w2v embedding dimensions  300\n",
      "len encoded notes, or total notes is  8000\n",
      "len of first note is  2629\n",
      "max len is  2629\n",
      "len or word index, or total unique words is  61122\n"
     ]
    }
   ],
   "source": [
    "print('pretrained GNV num embeddings ', len(embedding_matrix_GNV))\n",
    "print('GNV embedding dimensions ', len(embedding_matrix_GNV[0]))\n",
    "\n",
    "print('pretrained w2v num embeddings ', len(embedding_matrix_w2v))\n",
    "print('w2v embedding dimensions ', len(embedding_matrix_w2v[0]))\n",
    "\n",
    "print('len encoded notes, or total notes is ', len(input_ids))\n",
    "print('len of first note is ', len(input_ids[0]))\n",
    "print('max len is ', normal_max_len)\n",
    "print('len or word index, or total unique words is ', len(word2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41360dab-19f5-406d-9ecd-099fa40e5ec5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import (TensorDataset, DataLoader, RandomSampler, SequentialSampler)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def data_loader(x_train, x_test, y_train, y_test, batch_size=8):\n",
    "    \"\"\"Convert train and test sets to tensors and load them to a dataLoader\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert data type to torch.Tensor\n",
    "    x_train, x_test, y_train, y_test = tuple(torch.tensor(data) for data in [x_train, x_test, y_train, y_test])\n",
    "\n",
    "    # Create DataLoader for training data\n",
    "    train_data = TensorDataset(x_train, y_train)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    # Create DataLoader for validation data\n",
    "    val_data = TensorDataset(x_test, y_test)\n",
    "    val_sampler = SequentialSampler(val_data)\n",
    "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "    \n",
    " # Train Test Split\n",
    "x_train, x_test, y_train, y_test = train_test_split(input_ids, labels, test_size=0.33, random_state=seed)\n",
    "\n",
    "batch_size=8\n",
    "# Load data to PyTorch DataLoader\n",
    "train_dataloader, val_dataloader = data_loader(x_train, x_test, y_train, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a92bbb3c-2292-4bf7-bc6d-1c6238a11ffe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 pretrained_embedding=None,\n",
    "                 freeze_embedding=True,\n",
    "                 vocab_size=None,\n",
    "                 embed_dim=300,\n",
    "                 num_classes=2,\n",
    "                 dropout=0.2):\n",
    "        \n",
    "        super(CNN, self).__init__()\n",
    "        self.name = \"cnn_model\"\n",
    "        # embeddings\n",
    "        # we have 2 different pretrained, always using pretrained for this analysis\n",
    "        if pretrained_embedding is not None:\n",
    "            self.vocab_size, self.embed_dim = pretrained_embedding.shape\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_embedding, freeze=freeze_embedding)\n",
    "        else:\n",
    "            self.embed_dim = embed_dim\n",
    "            self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=self.embed_dim, padding_idx=0)\n",
    "            \n",
    "        self.filters_out = 128\n",
    "        # Conv Network\n",
    "        self.conv1 =  nn.Conv1d(self.embed_dim, self.filters_out, kernel_size=5)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc1 = nn.Linear(self.filters_out,10)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        #output layer\n",
    "        # we are always using binomial, ICD9 is more prevalent than ICD10, use either to find tobacco use\n",
    "        if num_classes == 2:\n",
    "            #binary\n",
    "            self.fc2 = nn.Linear(10, 1)\n",
    "            self.out = nn.Sigmoid()\n",
    "        else:\n",
    "            self.fc2 = nn.Linear(10, num_classes)\n",
    "            self.out = nn.Softmax()         \n",
    "                \n",
    "        nn.init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.fc2.weight, nonlinearity='relu')\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \n",
    "        # get embeddings. Output shape: (b, max_len, embed_dim)\n",
    "        batch_size = input_ids.shape[0]\n",
    "        x = self.embedding(input_ids).float()\n",
    "        #print(x)\n",
    "        # Permute to match input shape requirement of nn.Conv1d. Output shape: (b, embed_dim, max_len)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "\n",
    "        #adaptive Max pooling. Output shape: (b, self.filters_out, 1)\n",
    "        #input is a,b,in output is a,b,out\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Output shape: (b, self.filters_out)\n",
    "        x = x.squeeze()\n",
    "        #x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        # final output activation function\n",
    "        x = self.out(x)\n",
    "        x = x.view(batch_size)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2ee99c7-d6a8-4d69-b8b7-6381910acc4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=2, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "        self.prev_validation_loss = np.inf\n",
    "        self.f1 = 0\n",
    "\n",
    "    def early_stop_min(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def early_stop_min_f1(self, f1):\n",
    "        if f1 >= self.f1:\n",
    "            self.f1 = f1\n",
    "            self.counter = 0\n",
    "        elif f1 < (self.f1 - self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.prev_validation_loss:\n",
    "            self.counter = 0\n",
    "        if validation_loss > (self.prev_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True \n",
    "        self.prev_validation_loss = validation_loss\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f7b36e5-577a-40ac-93be-69f4cd5e234f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, loss, filename='models/cnn_model_checkpoint.torch'):\n",
    "    torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                }, filename)\n",
    "\n",
    "def load_checkpoint(model, optimizer, filename='models/cnn_model_checkpoint.torch'):\n",
    "    # Note: Input model & optimizer should be pre-defined.  This routine only updates their states.\n",
    "    epoch = 0\n",
    "    if os.path.isfile(filename):\n",
    "        print(\"=> loading checkpoint '{}'\".format(filename))\n",
    "        checkpoint = torch.load(filename)\n",
    "        epoch = checkpoint['epoch']\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {}) loss {}\"\n",
    "                  .format(filename, checkpoint['epoch'], checkpoint['loss']))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(filename))\n",
    "\n",
    "    return model, optimizer, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c07662a2-b54c-490b-be4b-8d84dbdb4eba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_model(model, dataloader):\n",
    "    model.eval()\n",
    "    Y_pred = []\n",
    "    Y_scores = []\n",
    "    Y_true = []\n",
    "    batch_loss = []\n",
    "    all_target = []\n",
    "    with torch.no_grad():\n",
    "        for x, target in dataloader:\n",
    "            # your code here\n",
    "            target = target.float()\n",
    "            Y_true.append(target)\n",
    "            Y_score = model(x)\n",
    "            #print('calcing loss in eval',Y_score,target)\n",
    "            loss = criterion(Y_score, target)\n",
    "            #print('loss is', loss)\n",
    "            batch_loss.append(loss.item())\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            predicted = (Y_score > .5).int()\n",
    "            Y_pred.append(predicted)\n",
    "            Y_scores.append(Y_score)\n",
    "            all_target.append(target)\n",
    "        val_loss = np.mean(batch_loss)\n",
    "        #print('mean loss is', val_loss)\n",
    "        Y_pred = np.concatenate(Y_pred, axis=0)\n",
    "        Y_true = np.concatenate(Y_true, axis=0)\n",
    "        all_target = np.concatenate(all_target, axis=0)\n",
    "        #f1 precision recall accuracy\n",
    "        #print('Y_true len',len(Y_true), 'Y_pred len', len(Y_pred))\n",
    "        print('sum Y pred', sum(Y_pred), 'sum target', sum(all_target))\n",
    "        #print('all Y_scores', np.concatenate(Y_scores, axis=0))\n",
    "        p, r, f, _ = precision_recall_fscore_support(Y_true, Y_pred, average='weighted')\n",
    "        a = accuracy_score(Y_true, Y_pred)\n",
    "    \n",
    "    return val_loss, p, r, f, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56e28741-0c28-4f77-9760-109ecebafcce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TRAIN\n",
    "\n",
    "n_epochs = 50\n",
    "\n",
    "def train_model(model, train_dataloader, current_epoch=0, n_epochs=n_epochs, optimizer=None, criterion=None):\n",
    "\n",
    "    model.train() # prep model for training\n",
    "    last_epoch = 0\n",
    "    last_epoch_loss = 0\n",
    "    early_stopper = None\n",
    "    early_stopper = EarlyStopper(patience=2, min_delta=0.01)\n",
    "    for epoch in range(current_epoch, current_epoch + n_epochs):\n",
    "        last_epoch = epoch + 1\n",
    "        curr_epoch_loss = []\n",
    "        #count = 0\n",
    "        \n",
    "        for data, target in train_dataloader:\n",
    "            #if count > 1:\n",
    "            #    break\n",
    "            #count = 1\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            \n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            curr_epoch_loss.append(loss.item())\n",
    "            \n",
    "        last_epoch_loss = np.mean(curr_epoch_loss)\n",
    "        echo_out = f\"Epoch {last_epoch}: curr_epoch_loss={last_epoch_loss}\"\n",
    "        print(echo_out)\n",
    "        #if (epoch + 1) % 10 == 0:\n",
    "        val_loss, p, r, f1, a = eval_model(model, val_dataloader)\n",
    "        val_echo_out = 'Epoch: {} \\t Validation loss: {:.2f}, precision: {:.2f}, recall:{:.2f}, f1_score: {:.2f}, accuracy: {:.2f}'.format(epoch + 1, val_loss, p, r, f1, a)\n",
    "        print(val_echo_out)\n",
    "        \n",
    "        with open(model.name + \"train_out\", 'a') as f:\n",
    "            f.write(echo_out + '\\n')\n",
    "            f.write(val_echo_out + '\\n')\n",
    "                 \n",
    "        if early_stopper and early_stopper.early_stop_min_f1(f1):\n",
    "            print('EARLY STOP')\n",
    "            with open(model.name + \"train_out\", 'a') as f:\n",
    "                f.write('EARLY STOP\\n')\n",
    "            break\n",
    "            \n",
    "    return model, last_epoch, last_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49b9e8d-308e-439b-ad5f-d1e0e761c624",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: curr_epoch_loss=0.5858439897200954\n",
      "sum Y pred 0 sum target 675.0\n",
      "Epoch: 1 \t Validation loss: 0.54, precision: 0.55, recall:0.74, f1_score: 0.64, accuracy: 0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: curr_epoch_loss=0.4849683018762674\n",
      "sum Y pred 225 sum target 675.0\n",
      "Epoch: 2 \t Validation loss: 0.49, precision: 0.76, recall:0.78, f1_score: 0.73, accuracy: 0.78\n",
      "Epoch 3: curr_epoch_loss=0.42406491665030593\n",
      "sum Y pred 285 sum target 675.0\n",
      "Epoch: 3 \t Validation loss: 0.48, precision: 0.76, recall:0.78, f1_score: 0.74, accuracy: 0.78\n",
      "Epoch 4: curr_epoch_loss=0.3752488709207791\n",
      "sum Y pred 403 sum target 675.0\n",
      "Epoch: 4 \t Validation loss: 0.48, precision: 0.76, recall:0.78, f1_score: 0.76, accuracy: 0.78\n",
      "Epoch 5: curr_epoch_loss=0.3071693166979213\n",
      "sum Y pred 430 sum target 675.0\n",
      "Epoch: 5 \t Validation loss: 0.49, precision: 0.76, recall:0.78, f1_score: 0.76, accuracy: 0.78\n",
      "Epoch 6: curr_epoch_loss=0.24042308904341797\n",
      "sum Y pred 516 sum target 675.0\n",
      "Epoch: 6 \t Validation loss: 0.50, precision: 0.77, recall:0.78, f1_score: 0.77, accuracy: 0.78\n",
      "Epoch 7: curr_epoch_loss=0.18374538474730145\n",
      "sum Y pred 385 sum target 675.0\n",
      "Epoch: 7 \t Validation loss: 0.52, precision: 0.77, recall:0.78, f1_score: 0.76, accuracy: 0.78\n",
      "Epoch 8: curr_epoch_loss=0.1253882726181799\n",
      "sum Y pred 554 sum target 675.0\n",
      "Epoch: 8 \t Validation loss: 0.53, precision: 0.76, recall:0.78, f1_score: 0.77, accuracy: 0.78\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "continued = False\n",
    "\n",
    "if continued:\n",
    "    with open('models/cnn_model.pckl', 'rb') as f:\n",
    "        cnn_model = pickle.load(f)\n",
    "    #cnn_optimizer = torch.optim.Adam(cnn_model.parameters(), lr=0.001, betas=(0.9, 0.999), weight_decay=1e-5, amsgrad=False)\n",
    "    cnn_model, cnn_optimizer, current_epoch = load_checkpoint(cnn_model, cnn_optimizer)\n",
    "else:\n",
    "    cnn_model = CNN(pretrained_embedding=embedding_matrix_GNV)\n",
    "    #cnn_model = CNN(pretrained_embedding=embedding_matrix_w2v)\n",
    "    cnn_optimizer = torch.optim.Adam(cnn_model.parameters(), lr=0.0002, betas=(0.9, 0.999), weight_decay=1e-5, amsgrad=False)\n",
    "    current_epoch = 0\n",
    "    \n",
    "cnn_model, last_epoch, last_epoch_loss = train_model(cnn_model, train_dataloader, current_epoch=current_epoch, optimizer=cnn_optimizer, criterion=criterion)\n",
    "\n",
    "with open('models/cnn_model.pckl', 'wb') as f:\n",
    "    pickle.dump(cnn_model, f)\n",
    "\n",
    "save_checkpoint(cnn_model, cnn_optimizer, last_epoch, last_epoch_loss)\n",
    "\n",
    "\n",
    "#p, r, f, a = eval_model(cnn_model, val_dataloader)\n",
    "#print('Epoch: {} \\t Validation precision: {:.2f}, recall:{:.2f}, f1_score: {:.2f}, accuracy: {:.2f}'.format(n_epochs, p, r, f, a))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d75ca1-769a-4dcf-b794-b8edef2beac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5f6054-4667-4f24-9f0e-4a2eac8a1584",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
